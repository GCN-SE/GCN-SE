{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GCNSE.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ILxyrfR7AfYi",
        "j1F6reAUrAge",
        "Aaa6K2GSJjKl",
        "6jPQPL2NJYX1",
        "oTSNKSq7JFB9",
        "X6QbBnexJM-q",
        "zqOxLXktJQkf",
        "Eb_2obuNdEb-",
        "_bdznQ3Xi59w",
        "AQIQt3F3oGTA",
        "8A6dZZVyoAVe",
        "XSrDODoWoLD5",
        "eGswwmyw89jk",
        "bVOcs5zeJoAj",
        "toT1PJrtJ6Y-",
        "SFL0YvlOxYJC",
        "A2jzYumZyHLM",
        "GsIl77AFybjh",
        "8cg-UPS0ycxo",
        "ELDlqkodYDA-",
        "rUNRNLoVGdmA",
        "4WDWQ7TxGih3",
        "YMYfGOodaSIU",
        "9xcVrJIJEYvq",
        "GyQbhLfuX-eJ",
        "ES0Idgi30e8f",
        "Twr2dXUe9SuW",
        "YUvjFzlrYhQj"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCQkSqrFKn1N",
        "scrolled": true
      },
      "source": [
        "!pip install dgl-cu101\n",
        "!pip install dynamicgem\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIjOjLfUQq-Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CY34Mmha5WD1"
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "!ls /content/drive/'My Drive'/Colab\n",
        "!ln -s /content/drive/'My Drive'/Colab /content/Colab\n",
        "\n",
        "!ls -al"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czpeoA9e_SF"
      },
      "source": [
        "# %rm DBLPE_importance.npz\n",
        "# %ls -hl ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1xwf7NzDFuNQ"
      },
      "source": [
        "!cp /content/Colab/Clustering-RGCN/*.py .\n",
        "!cp /content/Colab/Clustering-RGCN/*.npz .\n",
        "!ls -al ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsSGHX_-mWUB"
      },
      "source": [
        "# !cp /content/Colab/Clustering-RGCN/DBLPE_importance_2011.npz .\n",
        "!ls -al ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5QApBmLhOGP"
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "from utils import encode_onehot\n",
        "from models import GCNLSTM,GCN,GAT,GraphSage,EGCN,LSTMGCN,RNNGCN,TRNNGCN\n",
        "# from models import RGCN,GCNLSTM,GCN,dgl_GCN,GAT,GraphSage,EGCN,LSTMGCN,RNNGCN,TRNNGCN\n",
        "\n",
        "import tensorflow\n",
        "# from dynamicgem.embedding.dynAERNN  import DynAERNN\n",
        "\n",
        "import dgl\n",
        "\n",
        "import scipy as sp\n",
        "import scipy.linalg as linalg\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.cluster.vq import kmeans,vq\n",
        "from scipy import stats  \n",
        "\n",
        "from sklearn.cluster import SpectralClustering\n",
        "from sklearn import metrics\n",
        "\n",
        "from itertools import permutations \n",
        "\n",
        "try:\n",
        "  import google.colab\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z5CMAPT6gafH"
      },
      "source": [
        "### Model Training & Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XQhTdRFI2VB"
      },
      "source": [
        "def one_hot(l,classnum=1): #classnum fix some special case\n",
        "    one_hot_l=np.zeros((len(l),max(l.max()+1,classnum)))\n",
        "    for i in range(len(l)):\n",
        "        one_hot_l[i][l[i]]=1\n",
        "    return one_hot_l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_w-ZeV-gYDI"
      },
      "source": [
        "def train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type):\n",
        "    t = time.time()\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    #print(features.shape)\n",
        "    output = model(features, adj)\n",
        "    \n",
        "\n",
        "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "    if model_type == \"RNNGCN_LL\":\n",
        "        # print(\">> Added norm: \", torch.norm(model.LL, p=1))\n",
        "        loss_train += 0.05*torch.norm(model.LL, p=1)\n",
        "    if model_type == \"RNNGCN_SE_decay\":\n",
        "        # print(\">> Added norm: \", torch.norm(model.LL, p=1))\n",
        "        loss_train += 0.05 *torch.norm(model.Lambda, p=2)\n",
        "\n",
        "    pred_labels=torch.argmax(output,axis=1)\n",
        "    acc_train = metrics.accuracy_score(pred_labels[idx_train].cpu().detach().numpy(),labels[idx_train].cpu().detach().numpy())\n",
        "    \n",
        "    model.train()\n",
        "    loss_train.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    #print(loss_train,acc_train)\n",
        "\n",
        "    #validation\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    \n",
        "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "    if model_type == \"RNNGCN_LL\":\n",
        "        loss_val += 0.05*torch.norm(model.LL, p=1)\n",
        "    \n",
        "    acc_val = metrics.accuracy_score(pred_labels[idx_val].cpu().detach().numpy(),labels[idx_val].cpu().detach().numpy())\n",
        "    #print(loss_val,acc_val)\n",
        "    '''\n",
        "    print('Epoch: {:04d}'.format(epoch+1),\n",
        "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
        "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
        "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
        "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
        "          'time: {:.4f}s'.format(time.time() - t))\n",
        "    \n",
        "    a.write('Epoch: {:04d}'.format(epoch+1)+' '+\n",
        "          'loss_train: {:.4f}'.format(loss_train.item())+' '+\n",
        "          'acc_train: {:.4f}'.format(acc_train.item())+' '+\n",
        "          'loss_val: {:.4f}'.format(loss_val.item())+' '+\n",
        "          'acc_val: {:.4f}'.format(acc_val.item())+' '+\n",
        "          'time: {:.4f}s'.format(time.time() - t)+'\\n')\n",
        "    a.close()\n",
        "    '''\n",
        "    return acc_val"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRjSx8U6gSoo"
      },
      "source": [
        "def test(model, features, adj, labels, idx_test):\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    pred_labels=torch.argmax(output,axis=1)\n",
        "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "    acc_test = metrics.accuracy_score(labels[idx_test].cpu().detach().numpy(), pred_labels[idx_test].cpu().detach().numpy())\n",
        "    f1_test=metrics.f1_score(labels[idx_test].cpu().detach().numpy(), pred_labels[idx_test].cpu().detach().numpy(),average='weighted')\n",
        "    auc_test=metrics.roc_auc_score(one_hot(labels[idx_test].cpu().detach().numpy()), output[idx_test].cpu().detach().numpy(),multi_class='ovr',average='weighted')\n",
        "    \n",
        "    return loss_test.item(), acc_test, f1_test, auc_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILxyrfR7AfYi"
      },
      "source": [
        "### Generating datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eGI0UEsy7-Og"
      },
      "source": [
        "def getNormLaplacian(W):\n",
        "\t\"\"\"input matrix W=(w_ij)\n",
        "\t\"compute D=diag(d1,...dn)\n",
        "\t\"and L=D-W\n",
        "\t\"and Lbar=D^(-1/2)LD^(-1/2)\n",
        "\t\"return Lbar\n",
        "\t\"\"\"\n",
        "\td=[np.sum(row) for row in W]\n",
        "\tD=np.diag(d)\n",
        "\tL=D-W\n",
        "\tDn=np.power(np.linalg.matrix_power(D,-1),0.5)\n",
        "\tLbar=np.dot(np.dot(Dn,L),Dn)\n",
        "\treturn Lbar\n",
        " \n",
        "def getKlargestEigVec(Lbar,k):\n",
        "\t\"\"\"input\n",
        "\t\"matrix Lbar and k\n",
        "\t\"return\n",
        "\t\"k largest eigen values and their corresponding eigen vectors\n",
        "\t\"\"\"\n",
        "\teigval,eigvec=linalg.eig(Lbar)\n",
        "\tdim=len(eigval)\n",
        " \n",
        "\t#find top k largest eigval\n",
        "\tdictEigval=dict(zip(eigval,range(0,dim)))\n",
        "\tkEig=np.sort(eigval)[::-1][:k]#[0:k]\n",
        "\tix=[dictEigval[k] for k in kEig]\n",
        "\treturn eigval[ix],eigvec[:,ix]\n",
        " \n",
        "def getKlargestSigVec(Lbar,k):\n",
        "\t\"\"\"input\n",
        "\t\"matrix Lbar and k\n",
        "\t\"return\n",
        "\t\"k largest singular values and their corresponding eigen vectors\n",
        "\t\"\"\"\n",
        "\tlsigvec,sigval,rsigvec=linalg.svd(Lbar)\n",
        "\tdim=len(sigval)\n",
        " \n",
        "\t#find top k largest left sigval\n",
        "\tdictSigval=dict(zip(sigval,range(0,dim)))\n",
        "\tkSig=np.sort(sigval)[::-1][:k]#[0:k]\n",
        "\tix=[dictSigval[k] for k in kSig]\n",
        "\treturn sigval[ix],lsigvec[:,ix]\n",
        "\n",
        "def checkResult(Lbar,eigvec,eigval,k):\n",
        "\t\"\"\"\n",
        "\t\"input\n",
        "\t\"matrix Lbar and k eig values and k eig vectors\n",
        "\t\"print norm(Lbar*eigvec[:,i]-lamda[i]*eigvec[:,i])\n",
        "\t\"\"\"\n",
        "\tcheck=[np.dot(Lbar,eigvec[:,i])-eigval[i]*eigvec[:,i] for i in range(0,k)]\n",
        "\tlength=[np.linalg.norm(e) for e in check]/np.spacing(1)\n",
        "\tprint(\"Lbar*v-lamda*v are %s*%s\" % (length,np.spacing(1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gDsL3QtFVE-q"
      },
      "source": [
        "#setting of data generation\n",
        "\n",
        "def generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector):\n",
        "    transit_matrix=[]\n",
        "    for i in range(class_num):\n",
        "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "        transit_matrix+=[transit_one]\n",
        "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "    #assign initial labels\n",
        "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "    labels=labels.to(dtype=torch.long)\n",
        "    #label_node, speed up the generation of edges\n",
        "    label_node_dict=dict()\n",
        "\n",
        "    for j in range(class_num):\n",
        "        label_node_dict[j]=[]\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        label_node_dict[int(labels[i])]+=[int(i)]\n",
        "\n",
        "\n",
        "    #generate graph\n",
        "    for i in range(int(Time_steps)):\n",
        "        change_nodes=[]\n",
        "        # for all node labels:\n",
        "        for j in range(len(labels)):\n",
        "            if random.random()<epsilon_vector[labels[j]]:\n",
        "                #less than change probability\n",
        "                tmp=int(labels[j])\n",
        "                #print(j)\n",
        "                while(1): #change label\n",
        "                    labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
        "                    if labels[j]!=tmp:\n",
        "                        change_nodes+=[j]\n",
        "                        break\n",
        "                \n",
        "        label_node_dict=dict()\n",
        "        for j in range(class_num):\n",
        "            label_node_dict[j]=[]\n",
        "\n",
        "        for j in range(len(labels)):\n",
        "            label_node_dict[int(labels[j])]+=[int(j)]\n",
        "        #\n",
        "        #generate symmetrix adj matrix at each time step\n",
        "        for node_id in range(number_of_nodes):\n",
        "            j=labels[node_id]\n",
        "            for l in label_node_dict:\n",
        "                if l==j:\n",
        "                    for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                        if z>node_id and random.random()<link_inclass_prob:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "                else:\n",
        "                    for z in label_node_dict[l]:\n",
        "                        if z>node_id and random.random()<link_outclass_prob:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "                            \n",
        "\n",
        "\n",
        "    #generate feature use eye matrix\n",
        "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "    #seprate train,val,test\n",
        "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "    #probability matrix at last time_step\n",
        "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "    for j in range(number_of_nodes):\n",
        "        for k in range(number_of_nodes):\n",
        "          if j==k:\n",
        "                continue\n",
        "          elif labels[j]==labels[k]:\n",
        "            Probability_matrix[j][k]=link_inclass_prob\n",
        "          else:\n",
        "            Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "    return features.float(), adj.float(), labels, idx_train, idx_val, idx_test, Probability_matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Bz8QFGdAkEt"
      },
      "source": [
        "### Train and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCrl5TV_eg5t"
      },
      "source": [
        "\n",
        "def single_train_and_test(lambda_matrix, Probability_matrix, features, adj, labels, idx_train, idx_val, idx_test, model_type,normalize=False):\n",
        "\n",
        "    if model_type=='SPEC' or model_type=='SPEC_sklearn':\n",
        "        if type(lambda_matrix)!=type(None):\n",
        "            decay_adj=torch.zeros(adj.shape[0],adj.shape[2])\n",
        "            for j in range(adj.shape[0]):\n",
        "                for k in range(adj.shape[2]):\n",
        "                    decay_adj[j][k]=lambda_matrix[labels[j]][labels[k]]\n",
        "            now_adj=adj[:,0,:].clone()\n",
        "            for i in range(1,adj.shape[1]):  #time_steps\n",
        "                        tmp_adj=adj[:,i,:].clone()\n",
        "                        \n",
        "                        now_adj=(1-decay_adj)*now_adj+decay_adj*tmp_adj\n",
        "          \n",
        "            adj=now_adj\n",
        "        else:\n",
        "            now_adj=adj[:,0,:].clone()\n",
        "            for i in range(1,adj.shape[1]):  #time_steps\n",
        "                    now_adj+=adj[:,i,:].clone()\n",
        "            adj=now_adj\n",
        "        if normalize==True:\n",
        "            #normalize in both cases\n",
        "            \n",
        "            adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
        "            d=torch.sum(adj,axis=1)\n",
        "            D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
        "            D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
        "            adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
        "\n",
        "\n",
        "        Lbar=np.array(adj)  #no normalizaton\n",
        "        top_k=class_num\n",
        "        kSigVal,kSigVec=getKlargestSigVec(Lbar,top_k)\n",
        "        centroid=kmeans(kSigVec.astype(float),class_num)[0] #change kSigvec from complex64 to float\n",
        "        result=vq(kSigVec.astype(float),centroid)[0]\n",
        "\n",
        "        \n",
        "        perm = permutations(range(class_num)) \n",
        "        one_hot_result=torch.tensor(one_hot(result,class_num))\n",
        "        acc_test=0\n",
        "        f1_test=0\n",
        "        auc_test=0\n",
        "        count=0\n",
        "        for i in perm: \n",
        "            count+=1\n",
        "            one_hot_i=one_hot(np.array(i))\n",
        "            perm_result=torch.mm(one_hot_result,torch.tensor(one_hot_i))\n",
        "            pred_labels=torch.argmax(perm_result,axis=1)\n",
        "            acc_test = max(metrics.accuracy_score(labels,pred_labels),acc_test)\n",
        "            f1_test=max(metrics.f1_score(labels, pred_labels,average='weighted'),f1_test)\n",
        "            auc_test=max(metrics.roc_auc_score(one_hot(labels), perm_result,multi_class='ovr',average='weighted'),auc_test)\n",
        "            if count%10000==0:\n",
        "                print(count)\n",
        "                print(acc_test,f1_test,auc_test)   \n",
        "        print(str(acc_test)+'\\t'+str(f1_test)+'\\t'+str(auc_test))  \n",
        "        try:\n",
        "            spec_norm=getKlargestSigVec(adj-Probability_matrix,2)[0]\n",
        "        except:\n",
        "            spec_norm=[]\n",
        "        return 0,acc_test,spec_norm\n",
        "\n",
        "    elif model_type==\"DynAERNN\":\n",
        "        \n",
        "        length=adj.shape[1]\n",
        "        lookup=length-2\n",
        "\n",
        "        dim_emb  = class_num\n",
        "        if args_cuda:\n",
        "            tensorflow.device('/gpu:0')\n",
        "        embedding = DynAERNN(d   = dim_emb,\n",
        "            beta           = 5,\n",
        "            n_prev_graphs  = lookup,\n",
        "            nu1            = 1e-6,\n",
        "            nu2            = 1e-6,\n",
        "            n_aeunits      = [50, 30],\n",
        "            n_lstmunits    = [50,dim_emb],\n",
        "            rho            = 0.3,\n",
        "            n_iter         = args_epochs,\n",
        "            xeta           = 1e-3,\n",
        "            n_batch        = 10,\n",
        "            modelfile      = ['./intermediate/enc_model_dynAERNN.json', \n",
        "                              './intermediate/dec_model_dynAERNN.json'],\n",
        "            weightfile     = ['./intermediate/enc_weights_dynAERNN.hdf5', \n",
        "                              './intermediate/dec_weights_dynAERNN.hdf5'],\n",
        "            savefilesuffix = \"testing\")\n",
        "        embs = []\n",
        "        \n",
        "        graphs     = [nx.Graph(adj[:,l,:].numpy()) for l in range(length)]\n",
        "        for temp_var in range(lookup, length):\n",
        "                        emb, _ = embedding.learn_embeddings(graphs[:temp_var])\n",
        "                        embs.append(emb)\n",
        "        centroid=kmeans(embs[-1],class_num)[0] #change kSigvec from complex64 to float\n",
        "        result=vq(embs[-1],centroid)[0]\n",
        "\n",
        "        \n",
        "\n",
        "        perm = permutations(range(class_num)) \n",
        "        one_hot_result=torch.tensor(one_hot(result,class_num))\n",
        "        acc_test=0\n",
        "        f1_test=0\n",
        "        auc_test=0\n",
        "        count=0\n",
        "        for i in perm: \n",
        "            count+=1\n",
        "            one_hot_i=one_hot(np.array(i))\n",
        "            perm_result=torch.mm(one_hot_result,torch.tensor(one_hot_i))\n",
        "            pred_labels=torch.argmax(perm_result,axis=1)\n",
        "            acc_test = max(metrics.accuracy_score(labels,pred_labels),acc_test)\n",
        "            f1_test=max(metrics.f1_score(labels, pred_labels,average='weighted'),f1_test)\n",
        "            auc_test=max(metrics.roc_auc_score(one_hot(labels), perm_result,multi_class='ovr',average='weighted'),auc_test)\n",
        "            if count%10000==0:\n",
        "                print(count)\n",
        "                print(acc_test,f1_test,auc_test)   \n",
        "        print(str(acc_test)+'\\t'+str(f1_test)+'\\t'+str(auc_test))  \n",
        "        try:\n",
        "            spec_norm=getKlargestSigVec(adj-Probability_matrix,2)[0]\n",
        "        except:\n",
        "            spec_norm=[]\n",
        "        return 0,acc_test,spec_norm\n",
        "        \n",
        "\n",
        "\n",
        "    #choose adj matrix\n",
        "    #GCN:n*n, Others: n*t*n\n",
        "    attention_w = dict()\n",
        "    attention_w['simulated'] = [0.05907838, 0.05908475, 0.05908549, 0.0590926,  0.05907808, 0.05907955,\n",
        "                                0.05909814, 0.05977109, 0.06782387, 0.13779935, 0.16049674, 0.16051194]\n",
        "    attention_w[\"None\"] = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "    \n",
        "    attention_w[\"DBLP3\"] = [0.13257283, 0.11761762, 0.09207272, 0.09068672, \n",
        "                            0.13907257, 0.08077238, 0.07735384, 0.08926542, \n",
        "                            0.07579254, 0.10479335]\n",
        "    attention_w[\"DBLP5\"] = [0.08490249, 0.07058566, 0.07522732, 0.093458354, 0.1454923, 0.13680391, 0.08714645, 0.11872233, 0.12575606, 0.061905116]\n",
        "    attention_w[\"Reddit\"] = [0.10407882, 0.087858014, 0.103267744, 0.11869549, 0.111122675, 0.11745782, 0.09139953, 0.08404839, 0.089088246, 0.09298327]\n",
        "    attention_w[\"Brain\"] = [0.10703684, 0.08153439, 0.06159713, 0.10095412,\n",
        "                             0.05306721, 0.07350213, 0.12013485, 0.08990081,\n",
        "                             0.05808821, 0.10553119, 0.07727117, 0.07138196]\n",
        "\n",
        "    if model_type=='GCN':  \n",
        "        if type(lambda_matrix)!=type(None):\n",
        "            decay_adj=torch.zeros(adj.shape[0],adj.shape[0])\n",
        "            for j in range(adj.shape[0]):\n",
        "                for k in range(adj.shape[2]):\n",
        "                    decay_adj[j][k]=lambda_matrix[labels[j]][labels[k]]\n",
        "            now_adj=adj[:,0,:].clone()\n",
        "            \n",
        "            for i in range(1,adj.shape[1]):  #time_steps\n",
        "                tmp_adj=adj[:,i,:].clone()\n",
        "                now_adj=(1-decay_adj)*now_adj+decay_adj*tmp_adj\n",
        "            adj=now_adj\n",
        "        else:\n",
        "            now_adj=attention_w[dataset_name][0] * adj[:,0,:].clone()\n",
        "            for i in range(1,adj.shape[1]):  #time_steps\n",
        "                now_adj+= attention_w[dataset_name][i] * adj[:,i,:].clone()\n",
        "            adj=now_adj\n",
        "            \n",
        "        #normalize in both cases\n",
        "        # if normalize==True:\n",
        "        #     adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
        "        #     d=torch.sum(adj,axis=1)\n",
        "        #     D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
        "        #     D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
        "        #     adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
        "            \n",
        "        features=features[:,-1,:]\n",
        "          \n",
        "\n",
        "    elif model_type=='GAT' or model_type=='GraphSage':\n",
        "        # now_adj=adj[:,0,:].clone()\n",
        "        # for i in range(1,adj.shape[1]):  #time_steps\n",
        "        #     now_adj+=adj[:,i,:].clone()\n",
        "        # adj=now_adj\n",
        "        now_adj=attention_w[dataset_name][0] * adj[:,0,:].clone()\n",
        "        for i in range(1,adj.shape[1]):  #time_steps\n",
        "            now_adj+= attention_w[dataset_name][i] * adj[:,i,:].clone()\n",
        "        adj=now_adj\n",
        "        \n",
        "        #normalize in both cases\n",
        "        # if normalize==True:\n",
        "        #     adj+=torch.eye(adj.shape[0],adj.shape[1])\n",
        "        #     d=torch.sum(adj,axis=1)\n",
        "        #     D_minus_one_over_2=torch.zeros(adj.shape[0],adj.shape[0])\n",
        "        #     D_minus_one_over_2[range(len(D_minus_one_over_2)), range(len(D_minus_one_over_2))] = d**(-0.5)\n",
        "        #     adj=torch.mm(torch.mm(D_minus_one_over_2,adj),D_minus_one_over_2)\n",
        "            \n",
        "        features=features[:,-1,:]\n",
        "    elif model_type=='EGCN':\n",
        "        adj=torch.transpose(adj,0,1)\n",
        "        features=torch.transpose(features,0,1)\n",
        "        \n",
        "\n",
        "    #define model\n",
        "    if model_type=='GCN':\n",
        "        model = GCN(nfeat=features.shape[1],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type=='new_TRNNGCN':\n",
        "        model = new_TRNNGCN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                nnode=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "    elif model_type=='original_RNNGCN':\n",
        "        model = original_RNNGCN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type=='RNNGCN_2':               \n",
        "        model = RNNGCN_2(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "        \n",
        "    elif model_type=='RNNGCN_2_preNN':\n",
        "        model = RNNGCN_2_preNN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                use_cuda=args_cuda) \n",
        "                \n",
        "    elif model_type=='RNNGCN_1_preNN':\n",
        "        model = RNNGCN_1_preNN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                use_cuda=args_cuda)      \n",
        "\n",
        "    elif model_type=='RNNGCN_LL':\n",
        "        model = RNNGCN_LL(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=features.shape[1],\n",
        "                use_cuda=args_cuda)\n",
        "        \n",
        "    elif model_type=='RNNGCN_SE_decay':\n",
        "        model = RNNGCN_SE_decay(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)    \n",
        "\n",
        "    elif model_type=='RNNGCN_SE':\n",
        "        model = RNNGCN_SE(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "\n",
        "\n",
        "    elif model_type=='RNNGCN_SE_2ws':\n",
        "        print(\"building model: %s\" % model_type)\n",
        "        model = RNNGCN_SE_2ws(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "           \n",
        "\n",
        "    elif model_type=='RNNGCN_SE_back_pe':\n",
        "        print(\"building model: %s\" % model_type)\n",
        "        model = RNNGCN_SE_back_pe(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "           \n",
        "    elif model_type=='RNNGCN_SE_back':\n",
        "        print(\"building model: %s\" % model_type)\n",
        "        model = RNNGCN_SE_back(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "                \n",
        "    elif model_type=='TRNNGCN_SE':\n",
        "        model = TRNNGCN_SE(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                class_num=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)      \n",
        "\n",
        "    elif model_type=='TRNNGCN_SE_full':\n",
        "        model = TRNNGCN_SE_full(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                class_num=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)            \n",
        "    \n",
        "    elif model_type=='TRNNGCN_LL':\n",
        "        model = TRNNGCN_LL(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                class_num=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=adj.shape[1],\n",
        "                node_num=features.shape[0],\n",
        "                use_cuda=args_cuda)    \n",
        "                \n",
        "    elif model_type=='TRNNGCN_LL_exp':\n",
        "        model = TRNNGCN_LL_exp(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                nnode=features.shape[0],\n",
        "                tot_timestep=features.shape[1],\n",
        "                use_cuda=args_cuda)\n",
        "                \n",
        "    elif model_type=='RNNGCN_LL_exp':\n",
        "        model = RNNGCN_LL_exp(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                tot_timestep=features.shape[1],\n",
        "                use_cuda=args_cuda)\n",
        "                          \n",
        "                  \n",
        "    elif model_type=='RNNGCN':\n",
        "        model = RNNGCN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "        \n",
        "    elif model_type=='original_TRNNGCN':\n",
        "        model = original_TRNNGCN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout,\n",
        "                nnode=features.shape[0],\n",
        "                use_cuda=args_cuda)\n",
        "    elif model_type=='GCNLSTM':\n",
        "        model = GCNLSTM(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type=='RGCN':\n",
        "        model = RGCN(nfeat=features.shape[2],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type==\"GAT\":\n",
        "        adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
        "        model = GAT(nfeat=features.shape[1],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type==\"GraphSage\":\n",
        "        adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
        "        model = GraphSage(nfeat=features.shape[1],\n",
        "                nhid=args_hidden,\n",
        "                nclass=class_num,\n",
        "                dropout=args_dropout)\n",
        "    elif model_type==\"EGCN\":\n",
        "        model = EGCN(nfeat=features.shape[2],\n",
        "                    nhid=args_hidden,\n",
        "                    nclass=class_num,\n",
        "                    device=torch.device('cpu'))\n",
        "\n",
        "    \n",
        "        \n",
        "    if model_type!=\"SPEC\" and model_type!=\"SPEC_sklearn\" and model_type!=\"DynAERNN\":\n",
        "        if args_cuda:\n",
        "            if model_type != 'EGCN':\n",
        "                model=model.to(torch.device('cuda:0'))#.cuda()\n",
        "                features = features.cuda()\n",
        "                adj = adj.to(torch.device('cuda:0'))\n",
        "                labels = labels.cuda()\n",
        "                idx_train = idx_train.cuda()\n",
        "                idx_val = idx_val.cuda()\n",
        "                idx_test = idx_test.cuda()\n",
        "        #optimizer and train\n",
        "        optimizer = optim.Adam(model.parameters(),\n",
        "                              lr=args_lr, weight_decay=args_weight_decay)\n",
        "        # Train model\n",
        "        t_total = time.time()\n",
        "        best_val=0\n",
        "        for epoch in range(args_epochs):\n",
        "            acc_val=train(epoch, model, optimizer, features, adj, labels, idx_train, idx_val, model_type)\n",
        "            if epoch % 50 == 0 :\n",
        "                if model_type=='RNNGCN' or model_type=='TRNNGCN' or model_type=='new_TRNNGCN':\n",
        "                    print(\"epoch %d, Lambda:\" % epoch)\n",
        "                    print(model.Lambda)\n",
        "                if model_type=='RNNGCN_LL_exp':\n",
        "                    print(\"LL:\", model.LL)\n",
        "                    print(\"lambda:\", model.Lambda)\n",
        "                if model_type=='RNNGCN_SE_decay':\n",
        "                    print(\"lambda:\", model.Lambda)\n",
        "                if 'TRNNGCN_SE' in model_type:\n",
        "                    # print(\"matrix_weight:\", model.matrix_weight)    \n",
        "                    pass                \n",
        "            if acc_val>best_val:\n",
        "                best_val=acc_val\n",
        "                loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "                test_best_val=[loss,acc,auc,f1]\n",
        "            \n",
        "        # Testing\n",
        "        loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "        test_results = [loss, acc, auc, f1]\n",
        "        \n",
        "        SE_w = -1\n",
        "        matrix_w = -1\n",
        "        global SE_2ws_x\n",
        "        global SE_2ws_adj\n",
        "        \n",
        "        if model_type=='RNNGCN' or model_type=='TRNNGCN' or model_type=='new_TRNNGCN':\n",
        "            print(model.Lambda, end='\\t')\n",
        "        if model_type=='RNNGCN_SE_decay':\n",
        "            SE_w = model.attention_weight\n",
        "            print(model.Lambda, end='\\t')\n",
        "            print(\"attetion: \", model.attention_weight, end='\\t')  \n",
        "            print(\"c_weight: \", model.c_weight, end='\\t')      \n",
        "        if 'TRNNGCN_SE' in model_type:\n",
        "            matrix_w = model.matrix_weight\n",
        "        if 'RNNGCN_SE' in model_type:\n",
        "            if '2ws' in model_type:\n",
        "                SE_2ws_adj = model.channel_adj\n",
        "                SE_2ws_x = model.channel_x\n",
        "                print(\"adj: \", SE_2ws_adj, end='\\t')\n",
        "                print(\"x: \", SE_2ws_x, end='\\t')                \n",
        "                SE_w = model.channel_adj\n",
        "                # print(\"adj and x attetion: \", model.adj_x_2ws, end='\\t')  \n",
        "            else:\n",
        "                SE_w = model.attention_weight\n",
        "                print(\"attetion: \", model.attention_weight, end='\\t')  \n",
        "                print(\"c_weight: \", model.c_weight, end='\\t')               \n",
        "        if model_type=='RNNGCN_2':\n",
        "            print(model.x_Lambda, end='\\t')            \n",
        "            print(model.adj_Lambda, end='\\t')   \n",
        "        if model_type=='RNNGCN_LL':\n",
        "            print(model.LL, end='\\t')                  \n",
        "        if model_type=='RNNGCN_LL_exp':\n",
        "            print(model.LL_index, end='\\t')\n",
        "            print(model.LL_softmax, end='\\t')\n",
        "        if model_type=='TRNNGCN_LL_exp':\n",
        "            print(model.LL_index, end='\\t')\n",
        "            print(model.Lambda, end='\\t')\n",
        "        #print(loss,acc)\n",
        "        print(str(test_best_val[1])+'\\t'+str(test_best_val[2])+'\\t'+str(test_best_val[3]))#,end='\\t')\n",
        "        try:\n",
        "            spec_norm=getKlargestSigVec(now_adj-Probability_matrix,2)[0]\n",
        "        except:\n",
        "            spec_norm=0 #temperal adj\n",
        "            \n",
        "        # save model...\n",
        "        # print(\"Model's state_dict:\")\n",
        "        # for param_tensor in model.state_dict():\n",
        "        #     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "        if model_type == \"GCN\" or model_type == \"GraphSage\":\n",
        "            print(\"Saving model....\")\n",
        "            torch.save(model.state_dict(), \"./Eval_Mask_Model.tar\")\n",
        "\n",
        "        del model\n",
        "        return loss, acc, spec_norm, test_results, SE_w, matrix_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1F6reAUrAge"
      },
      "source": [
        "### Run Exp for Spectral Clustering and GCN with Decay Rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GA-ORtGokfVj"
      },
      "source": [
        "# def test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
        "#     for times in range(sample_time):     \n",
        "#         try:\n",
        "#             features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
        "#             for i in np.arange(0.0, 1.01, 0.01):\n",
        "#                 file_name='uncombined'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
        "#                           +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
        "#                           +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
        "#                           +\"sample_time_\"+str(sample_time)+\".txt\"\n",
        "#                 if IN_COLAB==True:\n",
        "#                     summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
        "#                 else:\n",
        "#                     summary_file = open(file_name,\"a+\")\n",
        "#                 t=time.time()\n",
        "#                 lambda_matrix=np.full((class_num,class_num),i)\n",
        "                \n",
        "#                 total_loss=0\n",
        "#                 total_acc=0\n",
        "#                 total_norm=[]\n",
        "#                 loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
        "\n",
        "#                 summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
        "#                         \"\\tTest set results:\" +\n",
        "#                         \"\\tloss= {:.6f}\".format(loss) + \n",
        "#                         \"\\taccuracy= {:.6f}\".format(acc)+\n",
        "#                         \"\\tspecnorm= {}\\n\".format(specnorm))\n",
        "#                 summary_file.close()\n",
        "#         except:\n",
        "#             error=1\n",
        "                \n",
        "# def test_epsilon_vector_kxklambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
        "#     for times in range(sample_time):     \n",
        "#         try:\n",
        "#             features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
        "#             for i in np.arange(0.0, 1.01, 0.1):\n",
        "#                 for j in np.arange(0, 1.01, 0.1):\n",
        "#                     for k in np.arange(0, 1.01, 0.1):\n",
        "#                         for l in np.arange(0, 1.01, 0.1):\n",
        "#                             file_name='uncombined'+'_'+'kxklambda'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
        "#                                           +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
        "#                                           +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
        "#                                           +\"sample_time_\"+str(sample_time)+\".txt\"\n",
        "#                             if IN_COLAB==True:\n",
        "#                                 summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
        "#                             else:\n",
        "#                                 summary_file = open(file_name,\"a+\")\n",
        "#                             t=time.time()\n",
        "#                             lambda_matrix=np.array([[i,j],[k,l]])\n",
        "#                             total_loss=0\n",
        "#                             total_acc=0\n",
        "#                             total_norm=[]\n",
        "#                             loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
        "\n",
        "#                             summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
        "#                                     \"\\tTest set results:\" +\n",
        "#                                     \"\\tloss= {:.6f}\".format(loss) + \n",
        "#                                     \"\\taccuracy= {:.6f}\".format(acc)+\n",
        "#                                     \"\\tspecnorm= {}\\n\".format(specnorm))\n",
        "                            \n",
        "#                             summary_file.close()\n",
        "#         except:\n",
        "#             error=1        \n",
        "            \n",
        "# def test_kxk_neural_network(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time):  \n",
        "#     for times in range(sample_time):     \n",
        "#         features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=generate_data(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector)               \n",
        "#         for i in range(1):\n",
        "#             file_name='uncombined'+'_'+'kxklambda'+'_'+model_type+\"_\" +\"number_of_nodes_\"+str(number_of_nodes)+'_' +\"Time_steps_\"+str(Time_steps)+'_'\\\n",
        "#                           +\"class_num_\"+str(class_num)+'_' +\"link_inclass_prob_\"+str(link_inclass_prob)+'_'\\\n",
        "#                           +\"link_outclass_prob_\"+str(link_outclass_prob)+'_'+\"epsilon_vector_\"+str(epsilon_vector)+'_'\\\n",
        "#                           +\"sample_time_\"+str(sample_time)+\".txt\"\n",
        "#             if IN_COLAB==True:\n",
        "#                 summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
        "#             else:\n",
        "#                 summary_file = open(file_name,\"a+\")\n",
        "#             t=time.time()\n",
        "#             lambda_matrix=np.full((class_num,class_num),0.2)\n",
        "#             #print(\"current matrix: {}\".format(lambda_matrix))\n",
        "#             total_loss=0\n",
        "#             total_acc=0\n",
        "#             total_norm=[]\n",
        "#             loss, acc, specnorm = single_train_and_test(lambda_matrix,Probability_matrix,features, adj, labels, idx_train, idx_val, idx_test, model_type)\n",
        "            \n",
        "#             summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
        "#                     \"\\tTest set results:\" +\n",
        "#                     \"\\tloss= {:.6f}\".format(loss) + \n",
        "#                     \"\\taccuracy= {:.6f}\".format(acc)+\n",
        "#                     \"\\tspecnorm= {}\\n\".format(specnorm))\n",
        "#             print(i,loss,acc,specnorm)\n",
        "#             #print(time.time()-t)\n",
        "#             summary_file.close()\n",
        "\n",
        "\n",
        "                        \n",
        "# #For simulated graphs\n",
        "\n",
        "# sample_time=100\n",
        "# number_of_nodes=200\n",
        "# Time_steps=500\n",
        "# class_num=2\n",
        "# link_inclass_prob=20/number_of_nodes/5  #when calculation , remove the link in itself\n",
        "# link_outclass_prob=link_inclass_prob/20\n",
        "\n",
        "# epsilon_vector=[10/number_of_nodes,20/number_of_nodes] # prob of changing labels\n",
        "\n",
        "\n",
        "\n",
        "# model_type='SPEC'    #GCN, GAT, GraphSage #SPEC(DynSPEC), DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "# args_hidden = class_num\n",
        "# args_dropout = 0.5\n",
        "# args_lr = 0.01\n",
        "# args_weight_decay = 5e-4\n",
        "# args_epochs = 250\n",
        "# args_no_cuda=False\n",
        "# args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##Different setting on simulated graphs\n",
        "\n",
        "# # test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "\n",
        "\n",
        "# # for number_of_nodes in [100,250,500]:\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # for link_inclass_prob in [10/number_of_nodes/5,20/number_of_nodes/5,30/number_of_nodes/5]:\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # for epsilon_vector in [[10/number_of_nodes,10/number_of_nodes],[20/number_of_nodes,20/number_of_nodes],[30/number_of_nodes,30/number_of_nodes],[40/number_of_nodes,40/number_of_nodes],[50/number_of_nodes,50/number_of_nodes],[60/number_of_nodes,60/number_of_nodes]]:\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # for Time_steps in [1000,2000,5000,10000]: #already have 500\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # for sample_time in [1,10,1000]: #already have 100\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # for epsilon_vector in [[10/number_of_nodes,20/number_of_nodes],[10/number_of_nodes,30/number_of_nodes],[20/number_of_nodes,30/number_of_nodes]]:\n",
        "# #    test_epsilon_vector_onelambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "# # # for epsilon_vector in [[10/number_of_nodes,20/number_of_nodes],[10/number_of_nodes,30/number_of_nodes],[20/number_of_nodes,30/number_of_nodes]]:\n",
        "# # for epsilon_vector in [[10/number_of_nodes,40/number_of_nodes],[20/number_of_nodes,40/number_of_nodes],[30/number_of_nodes,40/number_of_nodes]]:\n",
        "# #    test_epsilon_vector_kxklambda(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n",
        "\n",
        "\n",
        "# # test_kxk_neural_network(model_type,number_of_nodes,Time_steps,class_num,link_inclass_prob,link_outclass_prob, epsilon_vector,sample_time)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E7W0SiXm7T9w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXPVHbS4bhBB"
      },
      "source": [
        "### Editing datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDW5d64SnO9C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b60d5a35-f025-4002-cbc5-141389da2b67"
      },
      "source": [
        "new_d = np.load(\"Brain.npz\")\n",
        "for i in new_d.keys():\n",
        "    print(i)\n",
        "    print(new_d[i].shape)\n",
        "\n",
        "# print(new_d.files)\n",
        "adjs = new_d['adjs']\n",
        "attmats = new_d['attmats']\n",
        "labels = new_d['labels']\n",
        "\n",
        "period = 3\n",
        "for i in range(1, 4):\n",
        "    adjs[3*i:3*(i+1), ...] = adjs[:3, ...] \n",
        "    attmats[:, 3*i:3*(i+1), :] = attmats[:, :3, :]\n",
        "\n",
        "# adjs[5:10, ...] = adjs[:5, ...] \n",
        "# adjs[10:12, ...] = adjs[:2, ...] \n",
        "# attmats[:, 5:10, :] = attmats[:, :5, :]    \n",
        "# attmats[:, 10:12, :] = attmats[:, :2, :]    \n",
        "\n",
        "np.savez_compressed(\"3_Periodic_Brain.npz\", adjs=adjs, attmats=attmats, labels=labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adjs\n",
            "(12, 5000, 5000)\n",
            "attmats\n",
            "(5000, 12, 20)\n",
            "labels\n",
            "(5000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFPnV6q4KmXX"
      },
      "source": [
        "# # new_d = np.load(\"DBLPE.npz\")\n",
        "# # new_d = np.load(\"hospital.npz\")\n",
        "# new_d = np.load(\"DBLPE_importance.npz\")\n",
        "# for i in new_d.keys():\n",
        "#     print(i)\n",
        "#     print(new_d[i].shape)\n",
        "\n",
        "# # print(new_d.files)\n",
        "# adjs = new_d['adjs']\n",
        "# # attmats = new_d['attmats']\n",
        "# labels = new_d['labels']\n",
        "\n",
        "# print(labels)\n",
        "\n",
        "# # abnormal_indices = [3,8,9]\n",
        "\n",
        "# # for i in abnormal_indices:\n",
        "# #     print(adjs[i, ...])\n",
        "# #     adj_sp = adjs[i, ...]\n",
        "# #     indices = np.random.choice([True, False], adj_sp.shape, [0.1, 0.9])\n",
        "# #     indices = np.tril(indices, -1)\n",
        "# #     adj_sp[indices] = 0\n",
        "# #     adj_sp[indices.T] = 0\n",
        "\n",
        "# #     adjs[i, ...] = adj_sp\n",
        "# #     print(adjs[i, ...])\n",
        "\n",
        "# # np.savez_compressed(\"sparse_DBLPE.npz\", adjs=adjs, attmats=attmats, labels=labels)\n",
        "# # np.savez_compressed(\"sparse_DBLPE.npz\", adjs=adjs, labels=labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REsTzhwQUsQu"
      },
      "source": [
        "# ! ls\n",
        "# ! cp ./gpu_mem.log /content/Colab/Clustering-RGCN/\n",
        "# ! cp ./3_Periodic_Brain.npz /content/Colab/Clustering-RGCN/\n",
        "# ! cp ./sparse_DBLPE.npz /content/Colab/Clustering-RGCN/\n",
        "! ls -al  /content/Colab/Clustering-RGCN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nf2Z-uonKev6"
      },
      "source": [
        "# Run Exp on Simulated and Real Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKaGOwE8VIbJ"
      },
      "source": [
        "### Load and Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2i7evijEI2Vu"
      },
      "source": [
        "def load_real_data(dataset_name):\n",
        "    dataset_dict=dict()\n",
        "    dataset_dict[\"DBLP3\"]=\"DBLP3.npz\"\n",
        "    dataset_dict[\"DBLP5\"]=\"DBLP5.npz\"\n",
        "    dataset_dict[\"Brain\"]=\"Brain.npz\"\n",
        "    dataset_dict[\"Reddit\"]=\"reddit.npz\"\n",
        "    dataset_dict[\"DBLPE\"]=\"DBLPE.npz\"\n",
        "    dataset_dict[\"3_Periodic\"] = \"3_Periodic_Brain.npz\"\n",
        "    dataset_dict[\"5_Periodic\"] = \"5_Periodic_Brain.npz\"\n",
        "    dataset_dict[\"sparse_Brain\"] = \"sparse_Brain.npz\"\n",
        "    dataset_dict[\"sparse_DBLPE\"] = \"sparse_DBLPE.npz\"\n",
        "    dataset_dict[\"hospital\"] = \"hospital.npz\"\n",
        "    dataset_dict[\"political_retweet\"] = \"political_retweet.npz\"\n",
        "    dataset_dict[\"reality_call\"] = \"reality_call.npz\"\n",
        "    dataset_dict[\"DBLPE_importance\"] = \"DBLPE_importance.npz\"\n",
        "    \n",
        "    dataset      = np.load(dataset_dict[dataset_name])\n",
        "    \n",
        "    Graphs    = torch.LongTensor(dataset['adjs'])    #(n_time, n_node, n_node)\n",
        "    Graphs=torch.transpose(Graphs,0,1) #(n_node, n_time, n_node)\n",
        "\n",
        "    now_adj=Graphs[:,0,:].clone()\n",
        "    for i in range(1,Graphs.shape[1]):  #time_steps\n",
        "                  now_adj+=Graphs[:,i,:].clone()\n",
        "    d=torch.sum(now_adj,axis=1)\n",
        "    non_zero_index=torch.nonzero(d,as_tuple=True)[0]\n",
        "    Graphs=Graphs[non_zero_index,:,:]\n",
        "    Graphs=Graphs[:,:,non_zero_index]\n",
        "    \n",
        "    no_atts_list = [\"DBLPE\", \"DBLPE_importance\", \"sparse_DBLPE\"]\n",
        "    fixed_no_atts_list = [\"hospital\", \"political_retweet\", \"reality_call\"]\n",
        "    if dataset_name in fixed_no_atts_list:\n",
        "        Labels    = torch.LongTensor(np.argmax(dataset['labels'],axis=1))  #(n_node, num_classes) argmax        \n",
        "        Features=torch.zeros(Graphs.shape[0], Graphs.shape[1], Graphs.shape[2])\n",
        "        print(\"Features\", Features.shape)\n",
        "        for i in range(Features.shape[1]):\n",
        "            Features[:,i,:]=torch.eye(Features.shape[0],Features.shape[2])    \n",
        "        Labels=Labels[non_zero_index]\n",
        "    elif dataset_name in no_atts_list:\n",
        "        Labels = torch.LongTensor(np.argmax(dataset['labels'],axis=2))  #(n_node, n_time, num_classes) argmax\n",
        "        Features=torch.zeros(Graphs.shape[0], 1, Graphs.shape[2])\n",
        "        print(\"Features\", Features.shape)\n",
        "        for i in range(1):\n",
        "            Features[:,i,:]=torch.eye(Features.shape[0],Features.shape[2])      \n",
        "        #   for i in range(Features.shape[1]):\n",
        "        #       Features[:,i,:]=torch.eye(Features.shape[0],Features.shape[2])\n",
        "        Labels=Labels[non_zero_index]\n",
        "        \n",
        "    else:\n",
        "        Labels    = torch.LongTensor(np.argmax(dataset['labels'],axis=1))  #(n_node, num_classes) argmax\n",
        "        Features  = torch.LongTensor(dataset['attmats']) #(n_node, n_time, att_dim)\n",
        "    \n",
        "        Features=Features[non_zero_index]\n",
        "        Labels=Labels[non_zero_index]\n",
        "    \n",
        "\n",
        "    \n",
        "    #shuffle datasets\n",
        "    number_of_nodes=Graphs.shape[0]\n",
        "    nodes_id=list(range(number_of_nodes))\n",
        "    \n",
        "    random.shuffle(nodes_id)\n",
        "    idx_train = torch.LongTensor(nodes_id[:(7*number_of_nodes)//10])\n",
        "    idx_val = torch.LongTensor(nodes_id[(7*number_of_nodes)//10: (9*number_of_nodes)//10])\n",
        "    idx_test = torch.LongTensor(nodes_id[(9*number_of_nodes)//10: number_of_nodes])\n",
        "    \n",
        "    return Features.float(), Graphs.float(), Labels.long(), idx_train, idx_val, idx_test, []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQGUXDVCRvh4"
      },
      "source": [
        "def test_real_dataset():\n",
        "                  \n",
        "    file_name=dataset_name+'_'+model_type+\".txt\"\n",
        "    # print(\"filename:\", file_name)\n",
        "    if IN_COLAB==True:\n",
        "        # summary_file = open(\"/content/drive/My Drive/\"+file_name,\"a+\")\n",
        "        summary_file = open(\"/content/drive/My Drive/RNNGCN_results/\"+file_name,\"a+\")\n",
        "    else:\n",
        "        summary_file = open(file_name,\"a+\")\n",
        "    t=time.time()\n",
        "    lambda_matrix=None \n",
        "    total_loss=0\n",
        "    total_acc=0\n",
        "    total_norm=[]\n",
        "    loss, acc, specnorm, test_results, SE_w, matrix_w = single_train_and_test(lambda_matrix,Probability_matrix, features, adj, labels, idx_train, idx_val, idx_test, model_type,normalize=args_normalize)\n",
        "    if type(lambda_matrix)!=type(None):\n",
        "        summary_file.write(\"Weight decay: {}\".format(lambda_matrix.flatten()) +\n",
        "                                    \"\\tTest set results:\" +\n",
        "                                    \"\\tloss= {:.6f}\".format(loss) + \n",
        "                                    \"\\taccuracy= {:.6f}\".format(acc)+\n",
        "                                    \"\\tspecnorm= {}\\n\".format(specnorm))\n",
        "    else:\n",
        "        summary_file.write(\"Weight decay: {}\".format(0) +\n",
        "                                    \"\\tTest set results:\" +\n",
        "                                    \"\\tloss= {:.6f}\".format(loss) + \n",
        "                                    \"\\taccuracy= {:.6f}\".format(acc)+\n",
        "                                    \"\\tspecnorm= {}\\n\".format(specnorm))\n",
        "    \n",
        "    summary_file.close()\n",
        "    return test_results, SE_w, matrix_w"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aaa6K2GSJjKl"
      },
      "source": [
        "### Simulated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMHZprmsOTdl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jPQPL2NJYX1"
      },
      "source": [
        "### AH_GC\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgtaejfiBYB-"
      },
      "source": [
        "# import math\n",
        "\n",
        "# import torch\n",
        "\n",
        "# from torch.nn.parameter import Parameter\n",
        "# from torch.nn.modules.module import Module\n",
        "\n",
        "# class GraphConvolution_AH(Module):\n",
        "#     \"\"\"\n",
        "#     Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, in_features, out_features, bias=True):\n",
        "#         super(GraphConvolution_AH, self).__init__()\n",
        "#         self.in_features = in_features\n",
        "#         self.out_features = out_features\n",
        "#         self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "#         if bias:\n",
        "#             self.bias = Parameter(torch.FloatTensor(out_features))\n",
        "#         else:\n",
        "#             self.register_parameter('bias', None)\n",
        "#         self.reset_parameters()\n",
        "\n",
        "#     def reset_parameters(self):\n",
        "#         stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "#         self.weight.data.uniform_(-stdv, stdv)\n",
        "#         if self.bias is not None:\n",
        "#             self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "#     def forward(self, AH):\n",
        "#         # support = torch.mm(input, self.weight)\n",
        "#         # output = torch.spmm(adj, support)\n",
        "#         output = torch.spmm(AH, self.weight)\n",
        "#         if self.bias is not None:\n",
        "#             return output + self.bias\n",
        "#         else:\n",
        "#             return output\n",
        "\n",
        "#     def __repr__(self):\n",
        "#         return self.__class__.__name__ + ' (' \\\n",
        "#                + str(self.in_features) + ' -> ' \\\n",
        "#                + str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTSNKSq7JFB9"
      },
      "source": [
        "### Various Baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fEM-xEXFK9gW"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "import math\n",
        "from torch.nn.parameter import Parameter\n",
        "from layers import GraphConvolution\n",
        "\n",
        "from torch.autograd import Variable\n",
        "from dgl.nn.pytorch.conv import GraphConv,GATConv,SAGEConv\n",
        "import dgl\n",
        "\n",
        "def reset_parameters(self):\n",
        "        stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "class RNNGCN_1_preNN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, use_cuda=False):\n",
        "        super(RNNGCN_1_preNN, self).__init__()\n",
        "\n",
        "        nfeat_hid = nfeat\n",
        "        nfeat_out = nclass\n",
        "        # self.tot_timestep = adj.shape[1]\n",
        "\n",
        "        self.class_num = nclass\n",
        "        self.preNN0 = torch.nn.Sequential(nn.Linear(nfeat, nfeat_hid))\n",
        "        self.preNN1 = torch.nn.Sequential(nn.Linear(nfeat_hid, nfeat_hid))\n",
        "        self.preNN2 = torch.nn.Sequential(nn.Linear(nfeat_hid, nfeat_out))\n",
        "\n",
        "\n",
        "        # self.x_rnn = nn.RNN(input_size=nfeat,\n",
        "        #                     hidden_size=nfeat_hid,\n",
        "        #                     num_layers=2,\n",
        "        #                     batch_first=True)\n",
        "\n",
        "        # self.h_n = None\n",
        "        self.gc1 = GraphConvolution(nfeat_out, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.adj_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.adj_Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "        self.x_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.x_Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "        self.use_cuda=use_cuda\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        #out=[]\n",
        "        # x_in = Variable(torch.Tensor().type(torch.FloatTensor))\n",
        "        x_in = feats.clone()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            x_out = torch.zeros(feats.shape[0], feats.shape[1], self.class_num).cuda()\n",
        "        else:\n",
        "            x_out = torch.zeros(feats.shape[0], feats.shape[1], self.class_num)\n",
        "\n",
        "        tot_timestep = adj.shape[1]\n",
        "        for i in range(tot_timestep):\n",
        "            # print(\"x_in_i:\", x_in[:, i, :])\n",
        "            x_out_i = F.relu(self.preNN0(x_in[:, i, :]))\n",
        "            x_out_i = F.relu(self.preNN1(x_out_i))\n",
        "            x_out_i = self.preNN2(x_out_i)\n",
        "            x_out[:, i, :] = x_out_i\n",
        "        \n",
        "        \n",
        "        now_adj = adj[:,0,:].clone()\n",
        "        now_x = x_out[:,-1,:].clone()\n",
        "\n",
        "        for i in range(1, tot_timestep):  #time_steps\n",
        "            now_adj=(1-self.adj_Lambda)*now_adj+self.adj_Lambda*adj[:,i,:]  \n",
        "\n",
        "        one_out=self.gc1(now_x, now_adj)\n",
        "        one_out=F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "\n",
        "class RNNGCN_2_preNN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, use_cuda=False):\n",
        "        super(RNNGCN_2_preNN, self).__init__()\n",
        "\n",
        "        nfeat_hid = nfeat\n",
        "        nfeat_out = nclass\n",
        "        # self.tot_timestep = adj.shape[1]\n",
        "\n",
        "        self.preNN0 = torch.nn.Sequential(nn.Linear(nfeat, nfeat_hid))\n",
        "        self.preNN1 = torch.nn.Sequential(nn.Linear(nfeat_hid, nfeat_hid))\n",
        "        self.preNN2 = torch.nn.Sequential(nn.Linear(nfeat_hid, nfeat_out))\n",
        "\n",
        "\n",
        "        # self.x_rnn = nn.RNN(input_size=nfeat,\n",
        "        #                     hidden_size=nfeat_hid,\n",
        "        #                     num_layers=2,\n",
        "        #                     batch_first=True)\n",
        "\n",
        "        # self.h_n = None\n",
        "        self.gc1 = GraphConvolution(nfeat_out, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.adj_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.adj_Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "        self.x_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.x_Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "        self.use_cuda=use_cuda\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        #out=[]\n",
        "        # x_in = Variable(torch.Tensor().type(torch.FloatTensor))\n",
        "        x_in = feats.clone()\n",
        "\n",
        "        if self.use_cuda:\n",
        "            x_out = torch.zeros(np.shape(feats)).cuda()\n",
        "        else:\n",
        "            x_out = torch.zeros(np.shape(feats))\n",
        "\n",
        "        tot_timestep = adj.shape[1]\n",
        "        for i in range(tot_timestep):\n",
        "            # print(\"x_in_i:\", x_in[:, i, :])\n",
        "            x_out_i = F.relu(self.preNN0(x_in[:, i, :]))\n",
        "            x_out_i = F.relu(self.preNN1(x_out_i))\n",
        "            x_out_i = self.preNN2(x_out_i)\n",
        "            x_out[:, i, :] = x_out_i\n",
        "        \n",
        "        \n",
        "        now_adj = adj[:,0,:].clone()\n",
        "        now_x = x_out[:,0,:].clone()\n",
        "\n",
        "        for i in range(1, tot_timestep):  #time_steps\n",
        "            now_x = (1-self.x_Lambda)*now_x+self.x_Lambda*x_out[:, i, :]\n",
        "            \n",
        "        # separate two lambdas\n",
        "        # loss: sum up all time steps\n",
        "        # learn history bases\n",
        "            now_adj=(1-self.adj_Lambda)*now_adj+self.adj_Lambda*adj[:,i,:]  #weight decay\n",
        "\n",
        "        one_out=self.gc1(now_x, now_adj)\n",
        "        one_out=F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "class RNNGCN_2(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(RNNGCN_2, self).__init__()\n",
        "\n",
        "        nfeat_hid = nfeat\n",
        "        nfeat_out = nfeat\n",
        "\n",
        "        self.h_n = None\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.adj_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.adj_Lambda.data.uniform_(0.2, 0.2)\n",
        "\n",
        "        self.x_Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.x_Lambda.data.uniform_(0.2, 0.2)\n",
        "        \n",
        "    def forward(self, feats, adj):\n",
        "        #out=[]\n",
        "        # x_in = Variable(torch.Tensor().type(torch.FloatTensor))\n",
        "        x_out = feats.clone()\n",
        "\n",
        "        tot_timestep = adj.shape[1]\n",
        "        now_adj = adj[:,0,:].clone()\n",
        "        now_x = x_out[:,0,:].clone()\n",
        "\n",
        "        for i in range(1, tot_timestep):  #time_steps\n",
        "            now_x = (1-self.x_Lambda)*now_x+self.x_Lambda*x_out[:, i, :]\n",
        "        # separate two lambdas\n",
        "        # loss: sum up all time steps\n",
        "        # learn history bases\n",
        "            now_adj=(1-self.adj_Lambda)*now_adj+self.adj_Lambda*adj[:,i,:]  #weight decay\n",
        "\n",
        "        print(\"now_x: \", now_x.shape)\n",
        "        print(\"now_adj:\", now_adj.shape)\n",
        "        one_out=self.gc1(now_x, now_adj)\n",
        "        one_out=F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "class RNNGCN_RNN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(RNNGCN_RNN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.Lambda.data.uniform_(0.2, 0.2)\n",
        "        self.x_rnn = nn.RNN(input_size=nfeat,\n",
        "                    hidden_size=nfeat_hid,\n",
        "                    num_layers=2,\n",
        "                    batch_first=True)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        #out=[]\n",
        "        x_out, h_n = self.x_rnn(x, h_n)\n",
        "        now_adj=adj[:,0,:].clone()\n",
        "        for i in range(1,adj.shape[1]):  #time_steps\n",
        "            now_adj=(1-self.Lambda)*now_adj+self.Lambda*adj[:,i,:]  #weight decay\n",
        "        one_out=self.gc1(x[:,-1,:],now_adj)\n",
        "        one_out=F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out,now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "# class RNNGCN_LL(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, use_cuda=False):\n",
        "#         super(RNNGCN_LL, self).__init__()\n",
        "\n",
        "#         self.tot_timestep = tot_timestep\n",
        "\n",
        "#         self.LL = Parameter(torch.FloatTensor(self.tot_timestep))\n",
        "#         self.LL.data.uniform_(0.2, 0.2)\n",
        "        \n",
        "#         self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc2 = GraphConvolution(nhid, nclass)\n",
        "#         self.dropout = dropout\n",
        "\n",
        "#         self.use_cuda=use_cuda\n",
        "\n",
        "#     def forward(self, feats, adj):\n",
        "#         #out=[]\n",
        "#         # x_in = Variable(torch.Tensor().type(torch.FloatTensor))\n",
        "\n",
        "#         tot_timestep = adj.shape[1]\n",
        "#         assert tot_timestep == self.tot_timestep, \"timestep not consistent\"\n",
        "        \n",
        "#         # now_adj = adj[:,0,:].clone()\n",
        "\n",
        "#         now_adj = torch.zeros_like(adj[:,0,:])\n",
        "#         now_x = feats[:,-1,:].clone()\n",
        "\n",
        "#         for i in range(0, tot_timestep):  #time_steps\n",
        "#             now_adj += self.LL[i]*adj[:,i,:]  # weighted adj matrix\n",
        "\n",
        "#         # print(\"now_x: \", now_x.shape)\n",
        "#         # print(\"now_adj:\", now_adj.shape)\n",
        "#         one_out=self.gc1(now_x, now_adj)\n",
        "#         one_out=F.relu(one_out)\n",
        "\n",
        "#         one_out = F.dropout(one_out, self.dropout)\n",
        "#         one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "#         return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "\n",
        "class original_RNNGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
        "        super(original_RNNGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        self.Lambda.data.uniform_(0.2, 0.2)\n",
        "        \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "        #out=[]\n",
        "        now_adj=adj[:,0,:].clone()\n",
        "        for i in range(1,adj.shape[1]):  #time_steps\n",
        "            now_adj=(1-self.Lambda)*now_adj+self.Lambda*adj[:,i,:]  #weight decay\n",
        "        \n",
        "        one_out=self.gc1(x[:,-1,:],now_adj)\n",
        "        one_out=F.relu(one_out)\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out,now_adj)\n",
        "\n",
        "        return F.log_softmax(one_out, dim=1)\n",
        "\n",
        "# class new_TRNNGCN(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout,nnode,use_cuda=False):\n",
        "#         super(new_TRNNGCN, self).__init__()\n",
        "\n",
        "#         self.gc1 = GraphConvolution_AH(nfeat, nhid)\n",
        "#         self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        \n",
        "#         self.dropout = dropout\n",
        "#         self.Lambda = Parameter(torch.FloatTensor(nclass,nclass))\n",
        "#         self.Lambda.data.uniform_(0.5, 0.5)\n",
        "#         self.use_cuda=use_cuda\n",
        "        \n",
        "#         y=torch.randint(0,nclass,(nnode,1)).flatten()\n",
        "        \n",
        "#         if self.use_cuda:\n",
        "#             self.H = torch.zeros(nnode, nclass).cuda()\n",
        "#         else:\n",
        "#             self.H = torch.zeros(nnode, nclass)\n",
        "#         self.H[range(self.H.shape[0]), y]=1\n",
        "  \n",
        "        \n",
        "#     def forward(self, x, adj):\n",
        "\n",
        "#         w=self.Lambda.data\n",
        "#         w=w.clamp(0,1)\n",
        "#         self.Lambda.data=w\n",
        "#         if self.use_cuda:\n",
        "#             decay_adj=torch.mm(torch.mm(self.H,self.Lambda),self.H.T).cuda()\n",
        "#         else:\n",
        "#             decay_adj=torch.mm(torch.mm(self.H,self.Lambda),self.H.T)\n",
        "#         # -- H is theta\n",
        "\n",
        "#         tot_timestep = adj.shape[1]\n",
        "#         now_adj = adj[:,0,:].clone()\n",
        "#         now_x = x[:,0,:].clone()\n",
        "#         now_AH = torch.spmm(now_adj, now_x)\n",
        "\n",
        "#         # print(\"now_AH: \", now_AH.shape)\n",
        "#         # print(\"decay_adj: \", (1-decay_adj).shape)\n",
        "#         for i in range(1, tot_timestep):  #time_steps\n",
        "#             next_AH = torch.spmm(adj[:, i, :], x[:, i, :])\n",
        "#             now_AH = torch.spmm((1-decay_adj), now_AH) + torch.spmm(decay_adj, next_AH)\n",
        "#             # now_adj=(1-self.Lambda)*now_adj+self.Lambda*adj[:,i,:]  #weight decay\n",
        "\n",
        "#             now_adj=(1-decay_adj)*now_adj+decay_adj*adj[:,i,:]       \n",
        "#         # now_adj=adj[:,0,:].clone()#torch.zeros(adj.shape[0], adj.shape[2])\n",
        "#         # for i in range(1,adj.shape[1]):  #time_steps\n",
        "#         #     now_adj=(1-decay_adj)*now_adj+decay_adj*adj[:,i,:]\n",
        "#         # del decay_adj\n",
        "\n",
        "\n",
        "#         # one_out=F.relu(self.gc1(x[:,-1,:],now_adj))\n",
        "#         one_out=F.relu(self.gc1(now_AH))\n",
        "\n",
        "#         one_out = F.dropout(one_out, self.dropout)\n",
        "#         one_out = self.gc2(one_out,now_adj)\n",
        "#         # output=F.log_softmax(one_out, dim=1)\n",
        "#         output=F.softmax(one_out, dim=1)\n",
        "\n",
        "#         del self.H\n",
        "#         self.H = output.clone().detach()\n",
        "        \n",
        "#         del now_adj\n",
        "#         return output\n",
        "\n",
        "\n",
        "class original_TRNNGCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout,nnode,use_cuda=False):\n",
        "        super(original_TRNNGCN, self).__init__()\n",
        "\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        self.Lambda = Parameter(torch.FloatTensor(nclass,nclass))\n",
        "        self.Lambda.data.uniform_(0.5, 0.5)\n",
        "        self.use_cuda=use_cuda\n",
        "        \n",
        "        y=torch.randint(0,nclass,(nnode,1)).flatten()\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.H = torch.zeros(nnode, nclass).cuda()\n",
        "        else:\n",
        "            self.H = torch.zeros(nnode, nclass)\n",
        "        self.H[range(self.H.shape[0]), y]=1\n",
        "  \n",
        "        \n",
        "    def forward(self, x, adj):\n",
        "\n",
        "        w=self.Lambda.data\n",
        "        w=w.clamp(0,1)\n",
        "        self.Lambda.data=w\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            decay_adj=torch.mm(torch.mm(self.H,self.Lambda),self.H.T).cuda()\n",
        "        else:\n",
        "            decay_adj=torch.mm(torch.mm(self.H,self.Lambda),self.H.T)\n",
        "        \n",
        "        now_adj=adj[:,0,:].clone()#torch.zeros(adj.shape[0], adj.shape[2])\n",
        "       \n",
        "        for i in range(1,adj.shape[1]):  #time_steps\n",
        "            now_adj=(1-decay_adj)*now_adj+decay_adj*adj[:,i,:]\n",
        "        del decay_adj\n",
        "        one_out=F.relu(self.gc1(x[:,-1,:],now_adj))\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out,now_adj)\n",
        "        output=F.log_softmax(one_out, dim=1)\n",
        "        y=torch.argmax(output,dim=1)\n",
        "        H_shape=self.H.shape\n",
        "        del self.H\n",
        "        del now_adj\n",
        "        if self.use_cuda:\n",
        "            self.H = torch.zeros(H_shape).cuda()\n",
        "        else:\n",
        "            self.H = torch.zeros(H_shape)\n",
        "        self.H[range(H_shape[0]), y]=1\n",
        "        return output\n",
        "        \n",
        "class TRNNGCN_LL_exp(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, nnode, tot_timestep, use_cuda=False):\n",
        "        super(TRNNGCN_LL_exp, self).__init__()\n",
        "        self.nclass = nclass\n",
        "        self.nnode = nnode\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nclass)\n",
        "        self.dropout = dropout\n",
        "        self.Lambda = Parameter(torch.FloatTensor(nclass,nclass))\n",
        "        self.Lambda.data.uniform_(0.5, 0.5)\n",
        "\n",
        "        self.LL = Parameter(torch.FloatTensor(tot_timestep))\n",
        "        self.LL.data.uniform_()        \n",
        "        self.tot_timestep = tot_timestep\n",
        "\n",
        "        self.use_cuda=use_cuda\n",
        "        \n",
        "        y=torch.randint(0,nclass,(nnode,1)).flatten()\n",
        "        \n",
        "        # theta is membership matrix\n",
        "        if self.use_cuda:\n",
        "            self.theta = torch.zeros(nnode, nclass).cuda()\n",
        "        else:\n",
        "            self.theta = torch.zeros(nnode, nclass)\n",
        "        self.theta[range(self.theta.shape[0]), y]=1\n",
        "    def forward(self, x, adj):\n",
        "        # w=self.Lambda.data\n",
        "        # w=w.clamp(0,1)\n",
        "        # self.Lambda.data=w\n",
        "        if self.use_cuda:\n",
        "            lambda_matrix = torch.mm(torch.mm(self.theta,self.Lambda),self.theta.T).cuda()\n",
        "        else:\n",
        "            lambda_matrix = torch.mm(torch.mm(self.theta,self.Lambda),self.theta.T)\n",
        "        \n",
        "        now_adj = torch.zeros_like(adj[:,0,:])\n",
        "        # now_x = feats[:,-1,:].clone()\n",
        "\n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        self.LL_index = torch.topk(self.LL, k=mem_reach)[1]\n",
        "        # print(\"index: \", LL_index)\n",
        "        mem_sum = 0\n",
        "        \n",
        "        # print(\"lambda_matrix:\", lambda_matrix.shape)\n",
        "        for i in range(0, mem_reach):\n",
        "            mem_sum += torch.matrix_power(lambda_matrix + torch.eye(self.nnode).cuda(), i)\n",
        "        \n",
        "        \n",
        "        mem_sum = mem_sum.cuda()\n",
        "        self.LL_softmax = []\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            LL_exp = torch.matrix_power(lambda_matrix + torch.eye(self.nnode).cuda(), mem_reach-i) / mem_sum\n",
        "            self.LL_softmax.append(LL_exp)\n",
        "            LL_i = self.LL_index[i]\n",
        "            now_adj += LL_exp * adj[:,LL_i,:]  # weighted adj matrix\n",
        "\n",
        "        # now_adj=adj[:,0,:].clone()#torch.zeros(adj.shape[0], adj.shape[2])\n",
        "        # for i in range(1,adj.shape[1]):  #time_steps\n",
        "        #     now_adj=(1-decay_adj)*now_adj+decay_adj*adj[:,i,:]\n",
        "        # del decay_adj\n",
        "        one_out=F.relu(self.gc1(x[:,-1,:],now_adj))\n",
        "\n",
        "        one_out = F.dropout(one_out, self.dropout)\n",
        "        one_out = self.gc2(one_out,now_adj)\n",
        "        output=F.log_softmax(one_out, dim=1)\n",
        "        y=torch.argmax(output,dim=1)\n",
        "        theta_shape=self.theta.shape\n",
        "        del self.theta\n",
        "        del now_adj\n",
        "        if self.use_cuda:\n",
        "            self.theta = torch.zeros(theta_shape).cuda()\n",
        "        else:\n",
        "            self.theta = torch.zeros(theta_shape)\n",
        "        self.theta[range(theta_shape[0]), y]=1\n",
        "        return output \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6QbBnexJM-q"
      },
      "source": [
        "### RNNGCN_SE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m30g_f12wn8A"
      },
      "source": [
        "class RNNGCN_SE(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(RNNGCN_SE, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = nclass\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 0.5\n",
        "        self.attention_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "        self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "        self.last_linear = nn.Linear(nhid, nhid)\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # self.Lambda.data.uniform_()        \n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        #out=[]\n",
        "        # x_in = Variable(torch.Tensor().type(torch.FloatTensor))\n",
        "        tot_timestep = adj.shape[1]\n",
        "\n",
        "        if \"DBLPE\" in dataset_name:\n",
        "            # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "            # feats = feats[:, :tot_timestep, :]  \n",
        "            assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        else:\n",
        "            assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Consider timesteps as channels:\n",
        "        list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "            fixed_no_atts_list = [\"hospital\", \"reality_call\", \"retweet\"]\n",
        "            if \"DBLPE\" in dataset_name:\n",
        "                now_x = feats[:, -1, :] # identity matrix\n",
        "            elif dataset_name in fixed_no_atts_list:\n",
        "                now_x = feats[:, -1, :] # identity matrix\n",
        "            else:\n",
        "                now_x = feats[:,i,:]\n",
        "                \n",
        "            one_out=self.gc1(now_x, now_adj)\n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "            # squeezing:\n",
        "            list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            pooling_out = torch.mean(one_out)\n",
        "\n",
        "            list_c[i] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight = F.relu(self.W1(tensor_c))\n",
        "        c_weight = torch.sigmoid(self.W2(c_weight)) # or softmax\n",
        "        self.c_weight = c_weight\n",
        "\n",
        "        f_weight = c_weight.clone()\n",
        "        \n",
        "        # with_decay = 1\n",
        "        \n",
        "        # if with_decay:\n",
        "        #     w=self.Lambda.data\n",
        "        #     w=w.clamp(0.4, 0.99)\n",
        "        #     self.Lambda.data=w\n",
        "            \n",
        "        #     for i in range(mem_reach):\n",
        "        #         f_weight[i] = torch.mul(torch.pow(self.Lambda, mem_reach-i), c_weight[i])\n",
        "        \n",
        "        f_weight = F.softmax(f_weight, dim=0)\n",
        "\n",
        "        out = torch.zeros_like(tensor_u[0])\n",
        "        for i in range(mem_reach):\n",
        "            out += torch.mul(f_weight[i], tensor_u[i])\n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "\n",
        "        self.attention_weight = f_weight\n",
        "        out = self.last_linear(out)\n",
        "\n",
        "        del tensor_u\n",
        "        del tensor_c\n",
        "        del list_u\n",
        "        del list_c\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return F.log_softmax(out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lOvGvMqDvGwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqOxLXktJQkf"
      },
      "source": [
        "### RNNGCN_SE_back"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fFPjy-486noo"
      },
      "source": [
        "# Not considering the dynamic attributes;\n",
        "\n",
        "class RNNGCN_SE_back(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(RNNGCN_SE_back, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = nclass\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 0.5\n",
        "        self.attention_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)        \n",
        "        # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "        self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # self.Lambda.data.uniform_()        \n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        tot_timestep = adj.shape[1]\n",
        "\n",
        "        # if \"DBLPE\" in dataset_name:\n",
        "        #     # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "        #     # feats = feats[:, :tot_timestep, :]  \n",
        "        #     assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        # else:\n",
        "        #     assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        # assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Consider timesteps as channels:\n",
        "        list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "\n",
        "            # Normal one:\n",
        "            # if \"DBLPE\" in dataset_name:\n",
        "            #     now_x = feats[:, -1, :] # identity matrix\n",
        "            # else:\n",
        "            #     now_x = feats[:,i,:]\n",
        "\n",
        "            # For evaluation (with GCN):\n",
        "            now_x = feats[:, -1, :]\n",
        "\n",
        "            one_out=self.gc1(now_x, now_adj)\n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "            # squeezing:\n",
        "            list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            pooling_out = torch.mean(one_out)\n",
        "            list_c[i] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight = F.relu(self.W1(tensor_c))\n",
        "        c_weight = torch.sigmoid(self.W2(c_weight)) # or softmax\n",
        "        self.c_weight = c_weight\n",
        "\n",
        "        f_weight = c_weight.clone()\n",
        "          \n",
        "        f_weight = F.softmax(f_weight, dim=0)\n",
        "\n",
        "        adj_out = torch.zeros_like(adj[:,0,:]).cuda()\n",
        "        for i in range(mem_reach):        \n",
        "            adj_out += torch.mul(f_weight[i], adj[:,i,:])\n",
        "        \n",
        "        # x_out = torch.zeros_like(feats[:,0,:]).cuda()\n",
        "        # for i in range(mem_reach):        \n",
        "        #     x_out += torch.mul(f_weight[i], feats[:,i,:])            \n",
        "        \n",
        "        re_adj = adj_out\n",
        "        re_x = feats[:, -1, :]\n",
        "        # re_x = x_out\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        # re_out = self.gc1(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "        # re_out = self.gc2(re_out, re_adj)        \n",
        "        \n",
        "        re_out = self.last_linear(re_out)\n",
        "        \n",
        "        self.attention_weight = f_weight\n",
        "        del tensor_u\n",
        "        del tensor_c\n",
        "        del list_u\n",
        "        del list_c\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return F.log_softmax(re_out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEhimg4ukqbb"
      },
      "source": [
        "# # Considering an abstract of both atts and adjs, but with the same set of weight;\n",
        "\n",
        "# class RNNGCN_SE_back(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "#         super(RNNGCN_SE_back, self).__init__()\n",
        "#         self.tot_timestep = tot_timestep\n",
        "#         self.node_num = node_num\n",
        "#         self.nhid = nhid\n",
        "#         self.class_num = nclass\n",
        "\n",
        "#         self.dropout = dropout\n",
        "#         self.use_cuda=use_cuda\n",
        "#         self.C = self.tot_timestep # numbers of channel\n",
        "#         self.r = 0.5\n",
        "#         self.attention_weight = None\n",
        "#         self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "#         print(\"nhid:\", nhid)\n",
        "#         print(\"nfeat:\", nfeat)\n",
        "#         print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "#         self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "#         self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc4 = GraphConvolution(nhid, nhid)        \n",
        "#         # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "#         self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "#         self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "#         self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "#         # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "#         # self.Lambda.data.uniform_()        \n",
        "#         # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "#         # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "#         # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "#     def forward(self, feats, adj):\n",
        "#         tot_timestep = adj.shape[1]\n",
        "\n",
        "#         # if \"DBLPE\" in dataset_name:\n",
        "#         #     # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "#         #     # feats = feats[:, :tot_timestep, :]  \n",
        "#         #     assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "#         # else:\n",
        "#         #     assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "#         # assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "#         mem_reach = self.tot_timestep\n",
        "\n",
        "#         # Consider timesteps as channels:\n",
        "#         list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "#         list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "#         for i in range(0, mem_reach):  #time_steps\n",
        "#             now_adj = adj[:,i,:]\n",
        "#             if \"DBLPE\" in dataset_name:\n",
        "#                 now_x = feats[:, -1, :] # identity matrix\n",
        "#             else:\n",
        "#                 now_x = feats[:,i,:]\n",
        "#             one_out=self.gc1(now_x, now_adj)\n",
        "#             one_out = F.relu(one_out)\n",
        "#             one_out = F.dropout(one_out, self.dropout)\n",
        "#             one_out = self.gc2(one_out, now_adj)\n",
        "#             # squeezing:\n",
        "#             list_u[i, ...] = one_out\n",
        "#             # average pooling\n",
        "#             pooling_out = torch.mean(one_out)\n",
        "#             list_c[i] = pooling_out\n",
        "#             # del now_x, now_adj\n",
        "#             # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "#         tensor_u = list_u.cuda()\n",
        "#         tensor_c = list_c.cuda()\n",
        "\n",
        "#         # print(\"tensor_u: \", tensor_u.shape)\n",
        "#         # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "#         # exitation\n",
        "#         c_weight = F.relu(self.W1(tensor_c))\n",
        "#         c_weight = torch.sigmoid(self.W2(c_weight)) # or softmax\n",
        "#         self.c_weight = c_weight\n",
        "\n",
        "#         f_weight = c_weight.clone()\n",
        "          \n",
        "#         f_weight = F.softmax(f_weight, dim=0)\n",
        "\n",
        "#         adj_out = torch.zeros_like(adj[:,0,:]).cuda()\n",
        "#         for i in range(mem_reach):        \n",
        "#             adj_out += torch.mul(f_weight[i], adj[:,i,:])\n",
        "        \n",
        "#         x_out = torch.zeros_like(feats[:,0,:]).cuda()\n",
        "#         for i in range(mem_reach):        \n",
        "#             x_out += torch.mul(f_weight[i], feats[:,i,:])            \n",
        "#         # out = torch.mul(c_weight, tensor_u)\n",
        "        \n",
        "#         re_adj = adj_out\n",
        "#         # re_x = feats[:, -1, :]\n",
        "#         re_x = x_out\n",
        "\n",
        "#         re_out = self.gc3(re_x, re_adj)\n",
        "#         # re_out = self.gc1(re_x, re_adj)\n",
        "#         re_out = F.relu(re_out)\n",
        "#         re_out = F.dropout(re_out, self.dropout)\n",
        "#         re_out = self.gc4(re_out, re_adj)\n",
        "#         # re_out = self.gc2(re_out, re_adj)        \n",
        "        \n",
        "#         re_out = self.last_linear(re_out)\n",
        "        \n",
        "#         self.attention_weight = f_weight\n",
        "#         del tensor_u\n",
        "#         del tensor_c\n",
        "#         del list_u\n",
        "#         del list_c\n",
        "\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#         return F.log_softmax(re_out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eb_2obuNdEb-"
      },
      "source": [
        "### Transformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmTVp4MndHRk"
      },
      "source": [
        "# class RNNGCN_SE_trans(nn.Module):\n",
        "#     def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "#         super(RNNGCN_SE_trans, self).__init__()\n",
        "#         self.tot_timestep = tot_timestep\n",
        "#         self.node_num = node_num\n",
        "#         self.nhid = nhid\n",
        "#         self.class_num = nclass\n",
        "\n",
        "#         self.dropout = dropout\n",
        "#         self.use_cuda=use_cuda\n",
        "#         self.C = self.tot_timestep # numbers of channel\n",
        "#         self.r = 0.5\n",
        "#         self.attention_weight = None\n",
        "#         self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "#         print(\"nhid:\", nhid)\n",
        "#         print(\"nfeat:\", nfeat)\n",
        "#         print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "#         self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "#         self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "#         self.gc4 = GraphConvolution(nhid, nhid)        \n",
        "#         # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "#         self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "#         self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "#         self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "#         # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "#         # self.Lambda.data.uniform_()        \n",
        "#         # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "#         # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "#         # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "#     def forward(self, feats, adj):\n",
        "#         tot_timestep = adj.shape[1]\n",
        "\n",
        "#         # if \"DBLPE\" in dataset_name:\n",
        "#         #     # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "#         #     # feats = feats[:, :tot_timestep, :]  \n",
        "#         #     assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "#         # else:\n",
        "#         #     assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "#         # assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "#         mem_reach = self.tot_timestep\n",
        "\n",
        "#         # Consider timesteps as channels:\n",
        "#         list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "#         list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "#         for i in range(0, mem_reach):  #time_steps\n",
        "#             now_adj = adj[:,i,:]\n",
        "#             if \"DBLPE\" in dataset_name:\n",
        "#                 now_x = feats[:, -1, :] # identity matrix\n",
        "#             else:\n",
        "#                 now_x = feats[:,i,:]\n",
        "#             one_out=self.gc1(now_x, now_adj)\n",
        "#             one_out = F.relu(one_out)\n",
        "#             one_out = F.dropout(one_out, self.dropout)\n",
        "#             one_out = self.gc2(one_out, now_adj)\n",
        "#             # squeezing:\n",
        "#             list_u[i, ...] = one_out\n",
        "#             # average pooling\n",
        "#             pooling_out = torch.mean(one_out)\n",
        "#             list_c[i] = pooling_out\n",
        "#             # del now_x, now_adj\n",
        "#             # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "#         tensor_u = list_u.cuda()\n",
        "#         tensor_c = list_c.cuda()\n",
        "\n",
        "#         # print(\"tensor_u: \", tensor_u.shape)\n",
        "#         # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "#         # exitation\n",
        "#         c_weight = F.relu(self.W1(tensor_c))\n",
        "#         c_weight = torch.sigmoid(self.W2(c_weight)) # or softmax\n",
        "#         self.c_weight = c_weight\n",
        "\n",
        "#         f_weight = c_weight.clone()\n",
        "          \n",
        "#         f_weight = F.softmax(f_weight, dim=0)\n",
        "\n",
        "#         adj_out = torch.zeros_like(adj[:,0,:]).cuda()\n",
        "#         for i in range(mem_reach):        \n",
        "#             adj_out += torch.mul(f_weight[i], adj[:,i,:])\n",
        "        \n",
        "#         x_out = torch.zeros_like(feats[:,0,:]).cuda()\n",
        "#         for i in range(mem_reach):        \n",
        "#             x_out += torch.mul(f_weight[i], feats[:,i,:])            \n",
        "#         # out = torch.mul(c_weight, tensor_u)\n",
        "        \n",
        "#         re_adj = adj_out\n",
        "#         # re_x = feats[:, -1, :]\n",
        "#         re_x = x_out\n",
        "\n",
        "#         re_out = self.gc3(re_x, re_adj)\n",
        "#         # re_out = self.gc1(re_x, re_adj)\n",
        "#         re_out = F.relu(re_out)\n",
        "#         re_out = F.dropout(re_out, self.dropout)\n",
        "#         re_out = self.gc4(re_out, re_adj)\n",
        "#         # re_out = self.gc2(re_out, re_adj)        \n",
        "        \n",
        "#         re_out = self.last_linear(re_out)\n",
        "        \n",
        "#         self.attention_weight = f_weight\n",
        "#         del tensor_u\n",
        "#         del tensor_c\n",
        "#         del list_u\n",
        "#         del list_c\n",
        "\n",
        "#         torch.cuda.empty_cache()\n",
        "\n",
        "#         return F.log_softmax(re_out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bdznQ3Xi59w"
      },
      "source": [
        "### RNNGCN_SE_2ws"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bV-rsqXZi3l5"
      },
      "source": [
        "# Considering an abstract of both atts and adjs, but with the same set of weight;\n",
        "\n",
        "class RNNGCN_SE_2ws(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(RNNGCN_SE_2ws, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = nclass\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 0.5\n",
        "        self.attention_weight = None\n",
        "        self.channel_adj = None\n",
        "        self.channel_x = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)        \n",
        "        # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "        self.W1_adj = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2_adj = nn.Linear(self.hid_C, self.C)\n",
        "        \n",
        "        self.W1_x = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2_x = nn.Linear(self.hid_C, self.C)\n",
        "\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # self.Lambda.data.uniform_()        \n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        tot_timestep = adj.shape[1]\n",
        "\n",
        "        # if \"DBLPE\" in dataset_name:\n",
        "        #     # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "        #     # feats = feats[:, :tot_timestep, :]  \n",
        "        #     assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        # else:\n",
        "        #     assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        # assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Consider timesteps as channels:\n",
        "        # list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "            fixed_no_atts_list = [\"hospital\", \"reality_call\", \"political_retweet\"]\n",
        "            if \"DBLPE\" in dataset_name:\n",
        "                now_x = feats[:, -1, :] # identity matrix\n",
        "            elif dataset_name in fixed_no_atts_list:\n",
        "                now_x = feats[:, -1, :] # identity matrix\n",
        "            else:\n",
        "                now_x = feats[:,i,:]\n",
        "            one_out=self.gc1(now_x, now_adj)\n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "        \n",
        "            # list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            pooling_out = torch.mean(one_out)\n",
        "            list_c[i] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        # tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight_adj = F.relu(self.W1_adj(tensor_c))\n",
        "        c_weight_adj = torch.sigmoid(self.W2_adj(c_weight_adj)) # or softmax\n",
        "        f_weight_adj = c_weight_adj.clone()\n",
        "        channel_adj = F.softmax(f_weight_adj, dim=0)\n",
        "        self.channel_adj = channel_adj\n",
        "\n",
        "        c_weight_x = F.relu(self.W1_x(tensor_c))\n",
        "        c_weight_x = torch.sigmoid(self.W2_x(c_weight_x)) # or softmax\n",
        "        f_weight_x = c_weight_x.clone()\n",
        "        channel_x = F.softmax(f_weight_x, dim=0)\n",
        "        self.channel_x = channel_x\n",
        "\n",
        "        adj_out = torch.zeros_like(adj[:,0,:]).cuda()\n",
        "        for i in range(mem_reach):        \n",
        "            adj_out += torch.mul(channel_adj[i], adj[:,i,:])\n",
        "        \n",
        "        x_out = torch.zeros_like(feats[:,0,:]).cuda()\n",
        "        for i in range(mem_reach):        \n",
        "            x_out += torch.mul(channel_x[i], feats[:,i,:])            \n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "        \n",
        "        re_adj = adj_out\n",
        "        # re_x = feats[:, -1, :]    # Using the last feature for all time steps\n",
        "        re_x = x_out                # Using aggregated feature\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        # re_out = self.gc1(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "        # re_out = self.gc2(re_out, re_adj)        \n",
        "        \n",
        "        re_out = self.last_linear(re_out)\n",
        "        \n",
        "        del tensor_c\n",
        "        del list_c\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return F.log_softmax(re_out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQIQt3F3oGTA"
      },
      "source": [
        "### TRNNGCN_LL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BD9l08kmkE3S"
      },
      "source": [
        "class TRNNGCN_LL(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, class_num, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(TRNNGCN_LL, self).__init__()\n",
        "        self.c_weight = None\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = class_num\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 1.0\n",
        "        self.attention_weight = None\n",
        "        self.matrix_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (class_num, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)\n",
        "        \n",
        "        self.Lambda = Parameter(torch.FloatTensor(self.C, class_num,class_num))\n",
        "        self.Lambda.data.uniform_(0.5, 0.5)\n",
        "\n",
        "        self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "        # self.W_lambda = nn.Linear(self.C, 1)\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # if self.use_cuda:\n",
        "        #     self.H = torch.zeros(nnode, nclass).cuda()\n",
        "        # else:\n",
        "\n",
        "        y=torch.randint(0,class_num,(node_num,1)).flatten()\n",
        "        self.theta = torch.zeros(node_num, class_num).cuda()\n",
        "        self.theta[range(self.theta.shape[0]), y]=1\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.uniform_(self.Lambda.data, a=-1.0, b=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "        \n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        w=self.Lambda.data\n",
        "        w=w.clamp(0,1)\n",
        "        self.Lambda.data=w\n",
        "\n",
        "        class_num = self.class_num\n",
        "        node_num = self.node_num\n",
        "        if \"DBLPE\" in dataset_name:\n",
        "            # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "            # feats = feats[:, :tot_timestep, :]  \n",
        "            assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        else:\n",
        "            assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        tot_timestep = adj.shape[1]\n",
        "        assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "        \n",
        "        self.matrix_weight = self.Lambda\n",
        "\n",
        "        out = torch.zeros(node_num, node_num).cuda()\n",
        "        for i in range(mem_reach):\n",
        "            tmp_w = torch.mm(torch.mm(self.theta, self.Lambda[i, ...]),self.theta.T).cuda()            \n",
        "            out +=  tmp_w * adj[:,i,:]\n",
        "\n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "        re_adj = out\n",
        "        re_x = feats[:, -1, :]\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "\n",
        "        re_out = self.last_linear(re_out)\n",
        "\n",
        "        # del tensor_u\n",
        "        # del tensor_c\n",
        "        # del list_u\n",
        "        # del list_c\n",
        "\n",
        "        theta_shape = self.theta.shape\n",
        "        del self.theta\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        out_for_y = F.log_softmax(re_out, dim=1)\n",
        "        y = torch.argmax(out_for_y,dim=1)\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.theta = torch.zeros(theta_shape).cuda()\n",
        "        else:\n",
        "            self.theta = torch.zeros(theta_shape)\n",
        "        self.theta[range(theta_shape[0]), y]=1\n",
        "\n",
        "\n",
        "        return out_for_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8A6dZZVyoAVe"
      },
      "source": [
        "### TRNNGCN_SE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wn1UAbWBwdty"
      },
      "source": [
        "class TRNNGCN_SE(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, class_num, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(TRNNGCN_SE, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = class_num\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 1.0\n",
        "        self.attention_weight = None\n",
        "        self.matrix_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (class_num, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)\n",
        "        \n",
        "        self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C * class_num * class_num)\n",
        "        # self.W_lambda = nn.Linear(self.C, 1)\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # if self.use_cuda:\n",
        "        #     self.H = torch.zeros(nnode, nclass).cuda()\n",
        "        # else:\n",
        "\n",
        "        y=torch.randint(0,class_num,(node_num,1)).flatten()\n",
        "        self.theta = torch.zeros(node_num, class_num).cuda()\n",
        "        self.theta[range(self.theta.shape[0]), y]=1\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.uniform_(self.Lambda.data, a=-1.0, b=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "        \n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        class_num = self.class_num\n",
        "        node_num = self.node_num\n",
        "        if \"DBLPE\" in dataset_name:\n",
        "            # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "            # feats = feats[:, :tot_timestep, :]  \n",
        "            assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        else:\n",
        "            assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        tot_timestep = adj.shape[1]\n",
        "        assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Considering timesteps as channels:\n",
        "        list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "            if \"DBLPE\" in dataset_name:\n",
        "                now_x = feats[:, 0, :]\n",
        "            else:\n",
        "                now_x = feats[:,i,:]\n",
        "            one_out=self.gc1(now_x, now_adj)\n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "            # squeezing:\n",
        "            list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            pooling_out = torch.mean(one_out)\n",
        "\n",
        "            list_c[i] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight = F.relu(self.W1(tensor_c))\n",
        "        c_weight = torch.sigmoid(self.W2(c_weight)) \n",
        "        # c_weight = self.W2(c_weight)\n",
        "\n",
        "        # w=c_weight\n",
        "        # w=w.clamp(0,1)\n",
        "        # c_weight=w\n",
        "\n",
        "        # c_weight = F.elu(self.W2(c_weight)) + 1 # make sure c_weight > 0\n",
        "\n",
        "        # c_weight = F.softmax(c_weight, dim=0)\n",
        "\n",
        "        self.c_weight = c_weight\n",
        "\n",
        "        square_c = self.class_num * self.class_num\n",
        "        line_weight = torch.zeros((mem_reach, square_c)).cuda()\n",
        "        line_weight = c_weight.view(mem_reach, square_c)\n",
        "\n",
        "        # f_weight = F.softmax(f_weight, dim=0)\n",
        "        \n",
        "        \n",
        "        # norm_weight = torch.zeros(mem_reach).cuda()\n",
        "        # line_weight_normed = torch.zeros_like(line_weight).cuda()\n",
        "\n",
        "        line_weight_softmax = F.softmax(line_weight, dim=0)\n",
        "        \n",
        "        f_weight = torch.zeros((mem_reach, class_num, class_num)).cuda()\n",
        "        # for i in range(mem_reach):\n",
        "        #     f_weight[i, ...] = line_weight_softmax[i, ...].view(class_num, class_num)\n",
        "\n",
        "        for i in range(mem_reach):\n",
        "            f_weight[i, ...] = line_weight_softmax[i, ...].view(class_num, class_num)\n",
        "        \n",
        "        w_norm = torch.zeros(mem_reach).cuda()                \n",
        "        for i in range(mem_reach):\n",
        "            w_norm[i, ...] = torch.linalg.norm(f_weight[i, ...])\n",
        "            \n",
        "        softmax_norm = F.softmax(w_norm, dim=0)\n",
        "        self.attention_weight = softmax_norm\n",
        "        \n",
        "        # f_weight_clone = f_weight.clone()\n",
        "        # for i in range(mem_reach):\n",
        "        #     f_weight_normed[i, ...] = f_weight_clone[i, ...] / w_norm[i]\n",
        "        \n",
        "        self.matrix_weight = f_weight\n",
        "\n",
        "        out = torch.zeros(node_num, node_num).cuda()\n",
        "        for i in range(mem_reach):\n",
        "            tmp_w = torch.mm(torch.mm(self.theta, f_weight[i, ...]),self.theta.T).cuda()            \n",
        "            out +=  tmp_w * adj[:,i,:]\n",
        "\n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "        re_adj = out\n",
        "        re_x = feats[:, -1, :]\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "        \n",
        "\n",
        "        re_out = self.last_linear(re_out)\n",
        "\n",
        "        del tensor_u\n",
        "        del tensor_c\n",
        "        del list_u\n",
        "        del list_c\n",
        "\n",
        "        theta_shape = self.theta.shape\n",
        "        del self.theta\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        out_for_y = F.log_softmax(re_out, dim=1)\n",
        "        y = torch.argmax(out_for_y,dim=1)\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.theta = torch.zeros(theta_shape).cuda()\n",
        "        else:\n",
        "            self.theta = torch.zeros(theta_shape)\n",
        "        self.theta[range(theta_shape[0]), y]=1\n",
        "\n",
        "\n",
        "        return out_for_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSrDODoWoLD5"
      },
      "source": [
        "### TRNNGCN_SE_full"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbBS0mTTXf7X"
      },
      "source": [
        "class TRNNGCN_SE_full(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, class_num, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(TRNNGCN_SE_full, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = class_num\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 1.0\n",
        "        self.attention_weight = None\n",
        "        self.matrix_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (class_num, node_num))\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)\n",
        "        \n",
        "        self.W1 = nn.Linear(self.C * class_num, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C * class_num * class_num)\n",
        "        # self.W_lambda = nn.Linear(self.C, 1)\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # if self.use_cuda:\n",
        "        #     self.H = torch.zeros(nnode, nclass).cuda()\n",
        "        # else:\n",
        "\n",
        "        y=torch.randint(0,class_num,(node_num,1)).flatten()\n",
        "        self.theta = torch.zeros(node_num, class_num).cuda()\n",
        "        self.theta[range(self.theta.shape[0]), y]=1\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.uniform_(self.Lambda.data, a=-1.0, b=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "        \n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        class_num = self.class_num\n",
        "        node_num = self.node_num\n",
        "        if \"DBLPE\" in dataset_name:\n",
        "            # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "            # feats = feats[:, :tot_timestep, :]  \n",
        "            assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        else:\n",
        "            assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        tot_timestep = adj.shape[1]\n",
        "        assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Considering timesteps as channels:\n",
        "        list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep * self.class_num)\n",
        "\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "            if \"DBLPE\" in dataset_name:\n",
        "                now_x = feats[:, 0, :]\n",
        "            else:\n",
        "                now_x = feats[:,i,:]\n",
        "            one_out=self.gc1(now_x, now_adj)\n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "\n",
        "            # print(\"one_out\", one_out.shape)\n",
        "            # print(\"theta\", self.theta.shape)\n",
        "            one_out = one_out * self.theta\n",
        "            # squeezing:\n",
        "            list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            pooling_out = torch.mean(one_out, dim=0)\n",
        "            # print(\"pooling_out:\", pooling_out.shape)\n",
        "            list_c[i*self.class_num: (i+1)*self.class_num] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight = F.relu(self.W1(tensor_c))\n",
        "        c_weight = torch.sigmoid(self.W2(c_weight)) \n",
        "        # print(\"c_weight\", c_weight.shape)\n",
        "        # c_weight = self.W2(c_weight)\n",
        "\n",
        "        # w=c_weight\n",
        "        # w=w.clamp(0,1)\n",
        "        # c_weight=w\n",
        "\n",
        "        # c_weight = F.elu(self.W2(c_weight)) + 1 # make sure c_weight > 0\n",
        "\n",
        "        # c_weight = F.softmax(c_weight, dim=0)\n",
        "        \n",
        "\n",
        "        self.c_weight = c_weight\n",
        "\n",
        "        square_c = self.class_num * self.class_num\n",
        "        line_weight = torch.zeros((mem_reach, square_c)).cuda()\n",
        "        line_weight = c_weight.view(mem_reach, square_c)\n",
        "\n",
        "        # f_weight = F.softmax(f_weight, dim=0)\n",
        "        \n",
        "        \n",
        "        # norm_weight = torch.zeros(mem_reach).cuda()\n",
        "        \n",
        "        line_weight_softmax = F.softmax(line_weight, dim=0)\n",
        "        \n",
        "        f_weight = torch.zeros((mem_reach, class_num, class_num)).cuda()\n",
        "        # f_weight_normed = torch.zeros_like(f_weight).cuda()\n",
        "\n",
        "        for i in range(mem_reach):\n",
        "            f_weight[i, ...] = line_weight_softmax[i, ...].view(class_num, class_num)\n",
        "        \n",
        "        w_norm = torch.zeros(mem_reach).cuda()                \n",
        "        for i in range(mem_reach):\n",
        "            w_norm[i, ...] = torch.linalg.norm(f_weight[i, ...])\n",
        "\n",
        "        # for i in range(mem_reach):\n",
        "        #     f_weight_normed[i, ...] = f_weight[i, ...] / w_norm[i, ...]\n",
        "            \n",
        "        softmax_norm = F.softmax(w_norm, dim=0)\n",
        "        self.attention_weight = softmax_norm\n",
        "        self.matrix_weight = f_weight\n",
        "\n",
        "        out = torch.zeros(node_num, node_num).cuda()\n",
        "        for i in range(mem_reach):\n",
        "            tmp_w = torch.mm(torch.mm(self.theta, f_weight[i, ...]),self.theta.T).cuda()            \n",
        "            out +=  tmp_w * adj[:,i,:]\n",
        "        self.matrix_weight = f_weight\n",
        "\n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "        re_adj = out\n",
        "        re_x = feats[:, -1, :]\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "\n",
        "        re_out = self.last_linear(re_out)\n",
        "\n",
        "        del tensor_u\n",
        "        del tensor_c\n",
        "        del list_u\n",
        "        del list_c\n",
        "\n",
        "        theta_shape = self.theta.shape\n",
        "        del self.theta\n",
        "        # torch.cuda.empty_cache()\n",
        "\n",
        "        out_for_y = F.log_softmax(re_out, dim=1)\n",
        "        y = torch.argmax(out_for_y,dim=1)\n",
        "        \n",
        "        if self.use_cuda:\n",
        "            self.theta = torch.zeros(theta_shape).cuda()\n",
        "        else:\n",
        "            self.theta = torch.zeros(theta_shape)\n",
        "        self.theta[range(theta_shape[0]), y]=1\n",
        "\n",
        "\n",
        "        return out_for_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGswwmyw89jk"
      },
      "source": [
        "###  RNNGCN_SE_back_pe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jV5RISx88_m"
      },
      "source": [
        "class RNNGCN_SE_back_pe(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, tot_timestep, node_num, use_cuda=False):\n",
        "        super(RNNGCN_SE_back_pe, self).__init__()\n",
        "        self.tot_timestep = tot_timestep\n",
        "        self.node_num = node_num\n",
        "        self.nhid = nhid\n",
        "        self.class_num = nclass\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.use_cuda=use_cuda\n",
        "        self.C = self.tot_timestep # numbers of channel\n",
        "        self.r = 0.5\n",
        "        self.attention_weight = None\n",
        "        self.hid_C = int(self.C*self.r) + 1\n",
        "\n",
        "        print(\"nhid:\", nhid)\n",
        "        print(\"nfeat:\", nfeat)\n",
        "        print(\"class_num = %d, node_num = %d\" % (nclass, node_num))\n",
        "        self.pooling_nn = nn.Linear(nhid, 1)\n",
        "        self.gc1 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc2 = GraphConvolution(nhid, nhid)\n",
        "        self.gc3 = GraphConvolution(nfeat, nhid)\n",
        "        self.gc4 = GraphConvolution(nhid, nhid)        \n",
        "        # self.global_pooling = nn.AvgPool2d(self.node_num)\n",
        "        self.W1 = nn.Linear(self.C, self.hid_C)\n",
        "        self.W2 = nn.Linear(self.hid_C, self.C)\n",
        "        self.last_linear = nn.Linear(nhid, class_num)\n",
        "\n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # self.Lambda.data.uniform_()        \n",
        "        # self.Lambda = Parameter(torch.FloatTensor(1))\n",
        "        # torch.nn.init.normal_(self.Lambda.data, mean=0.0, std=1.0)\n",
        "        # print(\"lambda:\", self.Lambda.data)\n",
        "\n",
        "    def get_pe(self, max_len, d_model):\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        \n",
        "        j = 0\n",
        "        for i in range(1, d_model, 2):\n",
        "            pe[:, i] = torch.cos(position * div_term)[:, j]\n",
        "            j += 1\n",
        "        pe = pe.unsqueeze(0)\n",
        "        return pe\n",
        "\n",
        "    def forward(self, feats, adj):\n",
        "        tot_timestep = adj.shape[1]\n",
        "\n",
        "        # if \"DBLPE\" in dataset_name:\n",
        "        #     # print(\"feats:\", feats.shape, \"adj:\", adj.shape) \n",
        "        #     # feats = feats[:, :tot_timestep, :]  \n",
        "        #     assert feats.shape[1] == 1, \"feats shape is not 1, instead %d\" % feats.shape[1]\n",
        "        # else:\n",
        "        #     assert feats.shape[1] == adj.shape[1], \"feats and adj has different timestep\"\n",
        "        \n",
        "        # assert tot_timestep == self.tot_timestep, \"timestep not consistent, %d, %d\" % (tot_timestep, self.tot_timestep)\n",
        "     \n",
        "        mem_reach = self.tot_timestep\n",
        "\n",
        "        # Consider timesteps as channels:\n",
        "        list_u = torch.zeros((self.tot_timestep, self.node_num, self.nhid))\n",
        "        list_c = torch.zeros(self.tot_timestep)\n",
        "\n",
        "        pe = self.get_pe(mem_reach, self.nhid).cuda()\n",
        "        # print(\"pe:\", pe[0][0].shape)\n",
        "        for i in range(0, mem_reach):  #time_steps\n",
        "            now_adj = adj[:,i,:]\n",
        "            if \"DBLPE\" in dataset_name:\n",
        "                now_x = feats[:, -1, :] # identity matrix\n",
        "            else:\n",
        "                now_x = feats[:,i,:]\n",
        "            one_out = self.gc1(now_x, now_adj)\n",
        "\n",
        "            # one_out /= one_out.mean()\n",
        "            # print(\"one_out mean:\", one_out.mean())\n",
        "            # print(\"one_out:\", one_out)\n",
        "            one_out += pe[0][i]\n",
        "            \n",
        "            one_out = F.relu(one_out)\n",
        "            one_out = F.dropout(one_out, self.dropout)\n",
        "            one_out = self.gc2(one_out, now_adj)\n",
        "            # squeezing:\n",
        "            list_u[i, ...] = one_out\n",
        "            # average pooling\n",
        "            # pooling_out = self.pooling_nn(one_out)\n",
        "            pooling_out = torch.mean(one_out)\n",
        "            list_c[i] = pooling_out\n",
        "            # del now_x, now_adj\n",
        "            # one_out: node_num X tot_timestep X node_num\n",
        "\n",
        "        tensor_u = list_u.cuda()\n",
        "        tensor_c = list_c.cuda()\n",
        "\n",
        "        # print(\"tensor_u: \", tensor_u.shape)\n",
        "        # print(\"tensor_c: \", tensor_c.shape)\n",
        "\n",
        "        # exitation\n",
        "        c_weight = F.relu(self.W1(tensor_c))\n",
        "        c_weight = torch.sigmoid(self.W2(c_weight)) # or softmax\n",
        "        self.c_weight = c_weight\n",
        "\n",
        "        f_weight = c_weight.clone()\n",
        "          \n",
        "        f_weight = F.softmax(f_weight, dim=0)\n",
        "\n",
        "        out = torch.zeros_like(adj[:,0,:]).cuda()\n",
        "        for i in range(mem_reach):        \n",
        "            out += torch.mul(f_weight[i], adj[:,i,:])\n",
        "        # out = torch.mul(c_weight, tensor_u)\n",
        "        \n",
        "        re_adj = out\n",
        "        re_x = feats[:, -1, :]\n",
        "\n",
        "        re_out = self.gc3(re_x, re_adj)\n",
        "        # re_out = self.gc1(re_x, re_adj)\n",
        "        re_out = F.relu(re_out)\n",
        "        re_out = F.dropout(re_out, self.dropout)\n",
        "        re_out = self.gc4(re_out, re_adj)\n",
        "        # re_out = self.gc2(re_out, re_adj)        \n",
        "        \n",
        "        re_out = self.last_linear(re_out)\n",
        "        \n",
        "        self.attention_weight = f_weight\n",
        "        del tensor_u\n",
        "        del tensor_c\n",
        "        del list_u\n",
        "        del list_c\n",
        "\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return F.log_softmax(re_out, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EuAM1zB_cdf"
      },
      "source": [
        "# pe testing:\n",
        "\n",
        "# def get_pe(max_len, d_model):\n",
        "#     pe = torch.zeros(max_len, d_model)\n",
        "#     position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "#     div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "#     pe[:, 0::2] = torch.sin(position * div_term)\n",
        "#     pe[:, 1::2] = torch.cos(position * div_term)\n",
        "#     pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "#     return pe\n",
        "\n",
        "# pe = get_pe(4, 10)\n",
        "# print(pe)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVOcs5zeJoAj"
      },
      "source": [
        "### Count changed labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1PAALrWNqVm"
      },
      "source": [
        "# def get_label_stat(x):\n",
        "#     total_labels = x.clone().detach().cpu().numpy()\n",
        "#     print(total_labels.shape)\n",
        "#     res = []\n",
        "#     for i in range(1, total_labels.shape[1]): # timestep\n",
        "#         cur_labels = total_labels[:, i]\n",
        "#         pre_labels = total_labels[:, i-1]\n",
        "#         diff = np.abs(cur_labels - pre_labels) > 1e-5\n",
        "#         res.append(np.sum(diff))\n",
        "#     return np.array(res)\n",
        "\n",
        "# # get_label_stat(total_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toT1PJrtJ6Y-"
      },
      "source": [
        "### With same data sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWTNUnMDGE6u"
      },
      "source": [
        "# One data, multi runs\n",
        "\n",
        "\n",
        "# from gpu_memory_log import gpu_memory_log\n",
        "\n",
        "# target_res = []\n",
        "# SE_w_list = []\n",
        "# SE_w = 0\n",
        "\n",
        "\n",
        "# # dataset_name=\"DBLPE\"\n",
        "# # dataset_name=\"DBLPE_importance\"\n",
        "# # dataset_name=\"3_Periodic\"\n",
        "# # dataset_name=\"5_Periodic\"\n",
        "# # dataset_name=\"DBLP3\"\n",
        "# # dataset_name=\"DBLP5\"\n",
        "# dataset_name=\"Reddit\"\n",
        "# # dataset_name=\"Brain\"\n",
        "# # dataset_name=\"sparse_Brain\"\n",
        "# # dataset_name=\"sparse_DBLPE\"\n",
        "# # dataset_name=\"hospital\"\n",
        "\n",
        "# features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=load_real_data(dataset_name) \n",
        "\n",
        "# class_num=int(labels.max())+1\n",
        "\n",
        "# print(\"class_num =\", class_num)\n",
        "# total_adj=adj\n",
        "# total_labels=labels\n",
        "# print(\"total_adjsize: \", total_adj.shape)\n",
        "# print(\"total_labelsize: \", total_labels.shape)\n",
        "# print(\"features size: \", features.shape)\n",
        "\n",
        "\n",
        "# for i in range(10):\n",
        "\n",
        "#     torch.autograd.set_detect_anomaly(True)\n",
        "#     # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "\n",
        "#     # model_type = 'RNNGCN_1_preNN'    \n",
        "#     # model_type = 'RNNGCN_2_preNN'    \n",
        "#     # model_type = 'RNNGCN_LL'\n",
        "#     # model_type = 'RNNGCN_LL_exp'\n",
        "#     # model_type = 'RNNGCN_2'\n",
        "#     # model_type = 'RNNGCN_RNN'\n",
        "#     model_type = 'RNNGCN_SE_back'\n",
        "#     # model_type = 'RNNGCN_SE'\n",
        "#     # model_type = 'RNNGCN_SE_decay'\n",
        "#     # model_type = 'new_TRNNGCN' #AH_TRNNGCN\n",
        "#     # model_type = 'original_RNNGCN'\n",
        "#     # model_type = 'TRNNGCN_LL_exp'\n",
        "#     # model_type = 'original_TRNNGCN'\n",
        "#     # model_type = 'TRNNGCN_SE'\n",
        "#     # model_type = 'SPEC'\n",
        "#     # model_type = 'GCN'\n",
        "\n",
        "#     args_hidden = class_num\n",
        "#     args_dropout = 0.5\n",
        "#     args_weight_decay = 5e-4    \n",
        "    \n",
        "#     # args_lr = 0.0025\n",
        "#     args_lr = 0.0025\n",
        "    \n",
        "#     # args_epochs = 500\n",
        "#     args_epochs = 500 \n",
        "\n",
        "#     # args_no_cuda=True\n",
        "#     args_no_cuda=False\n",
        "    \n",
        "#     args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "#     print(\"CUDA:\", args_cuda)\n",
        "#     args_normalize=True\n",
        "\n",
        "#     print(dataset_name)\n",
        "\n",
        "#     # gpu_memory_log()\n",
        "#     if mode=='real':\n",
        "#         res_one, SE_w = test_real_dataset()\n",
        "#         target_res.append(res_one)\n",
        "#         if \"RNNGCN_SE\" in model_type:\n",
        "#             SE_w_list.append(SE_w.detach().cpu().numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-tmao_WGIKV"
      },
      "source": [
        "# while 1:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzNN2qh5lYvT"
      },
      "source": [
        "### Simulated Mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFL0YvlOxYJC"
      },
      "source": [
        "#### 2-stage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRtTEl0jgVK4"
      },
      "source": [
        "def generate_data_2stage(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep):\n",
        "    \n",
        "    transit_matrix=[]\n",
        "    for i in range(class_num):\n",
        "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "        transit_matrix+=[transit_one]\n",
        "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "    #assign initial labels\n",
        "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "    labels=labels.to(dtype=torch.long)\n",
        "    #label_node, speed up the generation of edges\n",
        "    label_node_dict=dict()\n",
        "\n",
        "    for j in range(class_num):\n",
        "        label_node_dict[j]=[]\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        label_node_dict[int(labels[i])]+=[int(i)]\n",
        "\n",
        "    total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
        "    \n",
        "    #generate graph\n",
        "    for i in range(int(Time_steps)):\n",
        "        #change node\n",
        "        change_nodes=[]\n",
        "        if i in special_timestep:    \n",
        "            for j in range(len(labels)):\n",
        "                # Only changing on special_timestep        \n",
        "                if random.random() < epsilon_vector[labels[j]]:\n",
        "                    #less than change probability\n",
        "                    tmp=int(labels[j])\n",
        "                    while(1): #change label\n",
        "                        labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
        "                        if labels[j]!=tmp:\n",
        "                            change_nodes+=[j]\n",
        "                            break\n",
        "                    #labels[j]=torch.tensor(not tmp)\n",
        "        total_labels[:,i]=labels.clone()\n",
        "        label_node_dict=dict()\n",
        "        for j in range(class_num):\n",
        "            label_node_dict[j]=[]\n",
        "\n",
        "        for j in range(len(labels)):\n",
        "            label_node_dict[int(labels[j])]+=[int(j)]\n",
        "        #\n",
        "        #generate symmetrix adj matrix at each time step\n",
        "        for node_id in range(number_of_nodes):\n",
        "            j=labels[node_id]\n",
        "            for l in label_node_dict:\n",
        "                if l==j:\n",
        "                    for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                        if z>node_id and random.random()<link_inclass_prob:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "                else:\n",
        "                    for z in label_node_dict[l]:\n",
        "                        if z>node_id and random.random()<link_outclass_prob:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "\n",
        "    #generate feature use eye matrix\n",
        "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "    #seprate train,val,test\n",
        "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "    #probability matrix at last time_step\n",
        "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "    for j in range(number_of_nodes):\n",
        "        for k in range(number_of_nodes):\n",
        "            if j==k:\n",
        "                continue\n",
        "            elif labels[j]==labels[k]:\n",
        "                Probability_matrix[j][k]=link_inclass_prob\n",
        "            else:\n",
        "                Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "    return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2jzYumZyHLM"
      },
      "source": [
        "#### Abnormal behavior"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZwanX_ryG4b"
      },
      "source": [
        "def generate_data_glitch(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep):\n",
        "    \n",
        "    transit_matrix=[]\n",
        "    for i in range(class_num):\n",
        "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "        transit_matrix+=[transit_one]\n",
        "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "    #assign initial labels\n",
        "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "    labels=labels.to(dtype=torch.long)\n",
        "\n",
        "    \n",
        "\n",
        "    total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
        "    #generate graph\n",
        "    for i in range(int(Time_steps)):\n",
        "        # print(\"for timestep %d, p = %.4f\" % (i, time_vector[i]))\n",
        "        # No label changing here\n",
        "        total_labels[:,i]=labels.clone()\n",
        "        \n",
        "        label_node_dict=dict()\n",
        "        for j in range(class_num):\n",
        "            label_node_dict[j]=[]\n",
        "        for j in range(len(labels)):\n",
        "            label_node_dict[int(labels[j])]+=[int(j)]\n",
        "\n",
        "        #generate symmetrix adj matrix at each time step\n",
        "        if i in special_timestep:\n",
        "            glitch_labels = torch.randint(0,class_num,(number_of_nodes,)).to(dtype=torch.long)\n",
        "            glitch_label_node_dict=dict()\n",
        "            for j in range(class_num):\n",
        "                glitch_label_node_dict[j]=[]\n",
        "            for j in range(len(glitch_labels)):\n",
        "                glitch_label_node_dict[int(glitch_labels[j])]+=[int(j)]\n",
        "        \n",
        "            for node_id in range(number_of_nodes):\n",
        "                j=glitch_labels[node_id]\n",
        "                for l in glitch_label_node_dict:\n",
        "                    if l==j:\n",
        "                        for z in glitch_label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                            if z>node_id and random.random()<link_inclass_prob:\n",
        "                                adj[node_id,i,z]= 1\n",
        "                                adj[z,i,node_id]= 1\n",
        "                    else:\n",
        "                        for z in glitch_label_node_dict[l]:\n",
        "                            if z>node_id and random.random()<link_outclass_prob:\n",
        "                                adj[node_id,i,z]= 1\n",
        "                                adj[z,i,node_id]= 1          \n",
        "        else:\n",
        "            for node_id in range(number_of_nodes):\n",
        "                j=labels[node_id]\n",
        "                for l in label_node_dict:\n",
        "                    if l==j:\n",
        "                        for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                            if z>node_id and random.random()<link_inclass_prob:\n",
        "                                adj[node_id,i,z]= 1\n",
        "                                adj[z,i,node_id]= 1\n",
        "                    else:\n",
        "                        for z in label_node_dict[l]:\n",
        "                            if z>node_id and random.random()<link_outclass_prob:\n",
        "                                adj[node_id,i,z]= 1\n",
        "                                adj[z,i,node_id]= 1\n",
        "\n",
        "    #generate feature use eye matrix\n",
        "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "    #seprate train,val,test\n",
        "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "    #probability matrix at last time_step\n",
        "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "    for j in range(number_of_nodes):\n",
        "        for k in range(number_of_nodes):\n",
        "            if j==k:\n",
        "                continue\n",
        "            elif labels[j]==labels[k]:\n",
        "                Probability_matrix[j][k]=link_inclass_prob\n",
        "            else:\n",
        "                Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "    return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GsIl77AFybjh"
      },
      "source": [
        "#### Fixed labels and Changing labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2OvnUL1ybVy"
      },
      "source": [
        "#simulated data: setting of data generation\n",
        "\n",
        "def generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector):\n",
        "    \n",
        "    transit_matrix=[]\n",
        "    for i in range(class_num):\n",
        "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "        transit_matrix+=[transit_one]\n",
        "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "    #assign initial labels\n",
        "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "    labels=labels.to(dtype=torch.long)\n",
        "    #label_node, speed up the generation of edges\n",
        "    label_node_dict=dict()\n",
        "\n",
        "    for j in range(class_num):\n",
        "        label_node_dict[j]=[]\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        label_node_dict[int(labels[i])]+=[int(i)]\n",
        "\n",
        "    total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
        "    #generate graph\n",
        "    for i in range(int(Time_steps)):\n",
        "        # print(\"for timestep %d, p = %.4f\" % (i, time_vector[i]))\n",
        "        #change node\n",
        "        change_nodes=[]\n",
        "        for j in range(len(labels)):\n",
        "            if random.random() < epsilon_vector[labels[j]]:\n",
        "                #less than change probability\n",
        "                tmp=int(labels[j])\n",
        "                #print(j)\n",
        "                while(1): #change label\n",
        "                    labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
        "                    if labels[j]!=tmp:\n",
        "                        change_nodes+=[j]\n",
        "                        break\n",
        "                #labels[j]=torch.tensor(not tmp)\n",
        "        total_labels[:,i]=labels.clone()\n",
        "        label_node_dict=dict()\n",
        "        for j in range(class_num):\n",
        "            label_node_dict[j]=[]\n",
        "\n",
        "        for j in range(len(labels)):\n",
        "            label_node_dict[int(labels[j])]+=[int(j)]\n",
        "        #\n",
        "        #generate symmetrix adj matrix at each time step\n",
        "        for node_id in range(number_of_nodes):\n",
        "            j=labels[node_id]\n",
        "            for l in label_node_dict:\n",
        "                if l==j:\n",
        "                    for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                        if z>node_id and random.random()<link_inclass_prob * time_vector[j, i]:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "                else:\n",
        "                    for z in label_node_dict[l]:\n",
        "                        if z>node_id and random.random()<link_outclass_prob * time_vector[j, i]:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "\n",
        "    #generate feature use eye matrix\n",
        "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "    #seprate train,val,test\n",
        "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "    #probability matrix at last time_step\n",
        "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "    for j in range(number_of_nodes):\n",
        "        for k in range(number_of_nodes):\n",
        "            if j==k:\n",
        "                continue\n",
        "            elif labels[j]==labels[k]:\n",
        "                Probability_matrix[j][k]=link_inclass_prob\n",
        "            else:\n",
        "                Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "    return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n",
        "\n",
        "\n",
        "#simulated data: setting of data generation\n",
        "\n",
        "def generate_data_totallabel_fixed(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector):\n",
        "    \n",
        "    transit_matrix=[]\n",
        "    for i in range(class_num):\n",
        "        transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "        transit_matrix+=[transit_one]\n",
        "    #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "    adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "    #assign initial labels\n",
        "    labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "    labels=labels.to(dtype=torch.long)\n",
        "    #label_node, speed up the generation of edges\n",
        "    label_node_dict=dict()\n",
        "\n",
        "    for j in range(class_num):\n",
        "        label_node_dict[j]=[]\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        label_node_dict[int(labels[i])]+=[int(i)]\n",
        "\n",
        "    total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
        "    #generate graph\n",
        "    for i in range(int(Time_steps)):\n",
        "        # print(\"for timestep %d, p = %.4f\" % (i, time_vector[i]))\n",
        "        # No label changing here\n",
        "        total_labels[:,i]=labels.clone()\n",
        "        \n",
        "        label_node_dict=dict()\n",
        "        for j in range(class_num):\n",
        "            label_node_dict[j]=[]\n",
        "\n",
        "        for j in range(len(labels)):\n",
        "            label_node_dict[int(labels[j])]+=[int(j)]\n",
        "        #\n",
        "        #generate symmetrix adj matrix at each time step\n",
        "        for node_id in range(number_of_nodes):\n",
        "            j=labels[node_id]\n",
        "            for l in label_node_dict:\n",
        "                if l==j:\n",
        "                    for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "                        if z>node_id and random.random()<link_inclass_prob * time_vector[j, i]:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "                else:\n",
        "                    for z in label_node_dict[l]:\n",
        "                        if z>node_id and random.random()<link_outclass_prob * time_vector[j, i]:\n",
        "                            adj[node_id,i,z]= 1\n",
        "                            adj[z,i,node_id]= 1\n",
        "\n",
        "    #generate feature use eye matrix\n",
        "    features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "    #seprate train,val,test\n",
        "    idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "    idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "    idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "    #probability matrix at last time_step\n",
        "    Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "    for j in range(number_of_nodes):\n",
        "        for k in range(number_of_nodes):\n",
        "            if j==k:\n",
        "                continue\n",
        "            elif labels[j]==labels[k]:\n",
        "                Probability_matrix[j][k]=link_inclass_prob\n",
        "            else:\n",
        "                Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "    return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cg-UPS0ycxo"
      },
      "source": [
        "#### Run on simulated graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fniDERogUQcs"
      },
      "source": [
        "# mode=\"simulated\"\n",
        "\n",
        "# if mode == 'simulated':   \n",
        "#     target_res = []\n",
        "#     SE_w_list = []\n",
        "#     SE_w = None\n",
        "#     matrix_w_list = []\n",
        "#     matrix_w = None\n",
        "#     SE_2ws_adj_list = []\n",
        "#     SE_2ws_x_list = []\n",
        "\n",
        "#     target_res_baseline = []\n",
        "#     SE_w_list_baseline = []\n",
        "#     matrix_w_list_baseline = []\n",
        "    \n",
        "#     for i in range(10):\n",
        "#         print(\"=\"*20)\n",
        "#         dataset_name='simulated'\n",
        "#         number_of_nodes=200\n",
        "#         Time_steps = 12\n",
        "#         class_num = 4\n",
        "#         link_inclass_prob = 20/number_of_nodes  #when calculation , remove the link in itself\n",
        "#         #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
        "\n",
        "#         link_outclass_prob=link_inclass_prob/20\n",
        "#         # epsilon_vector=[10/number_of_nodes,20/number_of_nodes, 40/number_of_nodes, 50/number_of_nodes]\n",
        "#         epsilon_vector=[0.05, 0.10, 0.20, 0.25]\n",
        "#         # epsilon_vector=[0.8, 0.8, 0.8, 0.8]\n",
        "\n",
        "#         # time_vector = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "#         special_timestep = [0,1,2,3,4]\n",
        "\n",
        "#         # time_vector = np.ones((class_num, Time_steps))\n",
        "#         # time_vector[0, 1] = 3.\n",
        "#         # time_vector[1, 1] = 3.\n",
        "#         # time_vector[2, 4] = 3.\n",
        "#         # time_vector[3, 4] = 3.\n",
        "        \n",
        "#         features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_glitch(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep)   \n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_2stage(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep)   \n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector)               \n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_totallabel_fixed(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector)               \n",
        "#         total_adj=adj\n",
        "#         total_labels=labels\n",
        "\n",
        "#         tot_timestep = total_labels.shape[1]\n",
        "#         print(\"adj:\", total_adj.shape)\n",
        "#         print(\"labels:\", total_labels.shape)\n",
        "#         print(\"timestep:\", tot_timestep)\n",
        "#         print(\"idx_train:\", idx_train.shape)\n",
        "#         print(\"idx_test:\", idx_test.shape)\n",
        "#         print(\"idx_val:\", idx_val.shape)\n",
        "\n",
        "#         args_hidden = class_num\n",
        "#         args_dropout = 0.5\n",
        "#         args_weight_decay = 5e-4    \n",
        "#         args_lr = 0.0025\n",
        "#         args_epochs = 500 \n",
        "#         # args_no_cuda=True\n",
        "#         args_no_cuda=False\n",
        "#         args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "#         print(\"CUDA:\", args_cuda)\n",
        "#         args_normalize=True\n",
        "#         print(dataset_name)\n",
        "\n",
        "#         # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "#         # model_type = 'RNNGCN_SE_2ws'\n",
        "#         model_type = 'RNNGCN_SE_back'\n",
        "\n",
        "#         for target_time in range(tot_timestep-1, tot_timestep):\n",
        "#             print(target_time,end='\\t')\n",
        "#             adj = total_adj[:,:target_time+1,:]\n",
        "#             labels = total_labels[:,target_time]\n",
        "      \n",
        "#             res_one, SE_w, matrix_w = test_real_dataset()\n",
        "#             target_res.append(res_one)\n",
        "#             if \"TRNNGCN_SE\" in model_type:\n",
        "#                 matrix_w_list.append(matrix_w.detach().detach().cpu().numpy())\n",
        "#             if \"RNNGCN_SE\" in model_type:\n",
        "#                 if \"2ws\" in model_type:\n",
        "#                     SE_2ws_adj_list.append(SE_2ws_adj.detach().cpu().numpy())\n",
        "#                     SE_2ws_x_list.append(SE_2ws_x.detach().cpu().numpy())\n",
        "#                 elif SE_w != None:\n",
        "#                     SE_w_list.append(SE_w.detach().cpu().numpy())            \n",
        "#             print(' ',end='\\n')\n",
        "\n",
        "#         # model_type = 'RNNGCN_SE_back'\n",
        "#         # for target_time in range(tot_timestep-1, tot_timestep):\n",
        "#         #     print(target_time,end='\\t')\n",
        "#         #     adj = total_adj[:,:target_time+1,:]\n",
        "#         #     labels = total_labels[:,target_time]\n",
        "      \n",
        "#         #     res_one, SE_w, matrix_w = test_real_dataset()\n",
        "#         #     target_res_baseline.append(res_one)\n",
        "#         #     if \"TRNNGCN_SE\" in model_type:\n",
        "#         #         matrix_w_list_baseline.append(matrix_w.detach().cpu().numpy())\n",
        "#         #     if \"RNNGCN_SE\" in model_type:\n",
        "#         #         if \"2ws\" in model_type:\n",
        "#         #             SE_2ws_adj_list.append(SE_2ws_adj.detach().cpu().numpy())\n",
        "#         #             SE_2ws_x_list.append(SE_2ws_x.detach().cpu().numpy())                \n",
        "#         #         elif SE_w != None:\n",
        "#         #             SE_w_list_baseline.append(SE_w.detach().cpu().numpy())            \n",
        "#         #     print(' ',end='\\n')        \n",
        "\n",
        "# SE_array = np.array(SE_w_list)\n",
        "# print(np.mean(SE_array, axis=0))\n",
        "# plt.plot(np.mean(SE_array, axis=0))\n",
        "# plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y-3OesIlq25S"
      },
      "source": [
        "\n",
        "# while 1:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RbBHAwQPUrZn"
      },
      "source": [
        "### Real Mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ1a6-01TSG5"
      },
      "source": [
        "mode = 'real'\n",
        "#\n",
        "if mode=='real':\n",
        "\n",
        "    SE_2ws_adj = None\n",
        "    SE_2ws_x = None\n",
        "\n",
        "    SE_2ws_adj_list = []\n",
        "    SE_2ws_x_list = []\n",
        "    target_res = []\n",
        "    SE_w_list = []\n",
        "    SE_w = None\n",
        "    matrix_w_list = []\n",
        "    matrix_w = None\n",
        "    target_res_baseline = []\n",
        "    SE_w_list_baseline = []\n",
        "    matrix_w_list_baseline = []\n",
        "\n",
        "    acc_drop_list = []\n",
        "\n",
        "    for i in range(5):\n",
        "        print(\"-\"*20)\n",
        "        # dataset_name=\"DBLPE\"\n",
        "        # dataset_name=\"DBLPE_importance\"\n",
        "        # dataset_name=\"3_Periodic\"\n",
        "        # dataset_name=\"5_Periodic\"\n",
        "        # dataset_name=\"DBLP3\"\n",
        "        dataset_name=\"DBLP5\"\n",
        "        # dataset_name=\"Reddit\"\n",
        "        # dataset_name=\"Brain\"\n",
        "        # dataset_name=\"sparse_Brain\"\n",
        "        # dataset_name=\"sparse_DBLPE\"\n",
        "        # dataset_name=\"hospital\"\n",
        "        # dataset_name=\"reality_call\"\n",
        "        # dataset_name=\"political_retweet\"\n",
        "\n",
        "        features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = load_real_data(dataset_name) \n",
        "        class_num=int(labels.max())+1\n",
        "        print(\"class_num =\", class_num)\n",
        "        total_adj=adj\n",
        "        total_labels=labels\n",
        "        print(\"total_adjsize: \", total_adj.shape)\n",
        "        print(\"total_labelsize: \", total_labels.shape)\n",
        "        print(\"features size: \", features.shape)\n",
        "\n",
        "        # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "        model_type = 'RNNGCN_SE_2ws'\n",
        "        # model_type = 'RNNGCN_SE_back'\n",
        "        # model_type = 'GraphSage'\n",
        "        # model_type = 'GCNLSTM'\n",
        "        # model_type = 'GCN'\n",
        "\n",
        "        args_hidden = class_num\n",
        "        # args_hidden = 10\n",
        "        args_dropout = 0.5\n",
        "        args_weight_decay = 5e-4    \n",
        "        args_lr = 0.0025\n",
        "        args_epochs = 500 \n",
        "        # args_no_cuda=True\n",
        "        args_no_cuda=False\n",
        "        args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "        print(\"CUDA:\", args_cuda)\n",
        "        args_normalize=True\n",
        "        print(dataset_name)\n",
        "        \n",
        "        no_atts_list = [\"DBLPE_importance\", \"DBLPE\", \"sparse_DBLPE\"]\n",
        "        fixed_no_atts_list = [\"hospital\", \"reality_call\", \"political_retweet\"]\n",
        "        \n",
        "        if dataset_name in fixed_no_atts_list:\n",
        "            res_one, SE_w, matrix_w = test_real_dataset()   \n",
        "            target_res.append(res_one)\n",
        "            if \"RNNGCN_SE\" in model_type:\n",
        "                if SE_w != None:\n",
        "                    SE_w_list.append(SE_w.detach().cpu().numpy())\n",
        "            print(' ',end='\\n')   \n",
        "         \n",
        "        elif dataset_name in no_atts_list:\n",
        "        # if dataset_name==\"DBLPE\" or dataset_name == \"sparse_DBLPE\":\n",
        "            #target_time=13 #0-13\n",
        "            for target_time in range(13,14):\n",
        "                # gpu_memory_log()\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "                print(target_time)\n",
        "                adj = total_adj[:,:target_time+1,:]\n",
        "                labels = total_labels[:,target_time]\n",
        "\n",
        "                res_one, SE_w, matrix_w = test_real_dataset()   \n",
        "                   \n",
        "                if \"RNNGCN_SE\" in model_type:\n",
        "                   if SE_w != None:\n",
        "                        SE_w_list.append(SE_w.detach().cpu().numpy())\n",
        "                target_res.append(res_one)\n",
        "                # gpu_memory_log()\n",
        "                print(' ',end='\\n')\n",
        "        else:\n",
        "            res_one, SE_w, matrix_w = test_real_dataset()\n",
        "            target_res.append(res_one)\n",
        "            # if \"TRNNGCN_SE\" in model_type:\n",
        "            #     matrix_w_list.append(matrix_w.detach().cpu().numpy())\n",
        "            if \"RNNGCN_SE\" in model_type:\n",
        "                if \"2ws\" in model_type:\n",
        "                    SE_2ws_adj_list.append(SE_2ws_adj.detach().cpu().numpy())               \n",
        "                    SE_2ws_x_list.append(SE_2ws_x.detach().cpu().numpy())               \n",
        "                elif SE_w != None:\n",
        "                    SE_w_list.append(SE_w.detach().cpu().numpy())\n",
        "        \n",
        "            # # Another one\n",
        "            # model_type = 'RNNGCN_SE_back'\n",
        "            # res_one, SE_w, matrix_w = test_real_dataset()\n",
        "            # target_res_baseline.append(res_one)\n",
        "            # if \"TRNNGCN_SE\" in model_type:\n",
        "            #     matrix_w_list_baseline.append(matrix_w.detach().cpu().numpy())\n",
        "            # if \"RNNGCN_SE\" in model_type:\n",
        "            #     if SE_w != None:\n",
        "            #         SE_w_list_baseline.append(SE_w.detach().cpu().numpy())            \n",
        "            # print(' ',end='\\n')                       \n",
        "        \n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNfsz_e0DYxq"
      },
      "source": [
        "plt.plot(np.mean(SE_2ws_x_list, axis=0))\n",
        "plt.plot(np.mean(SE_2ws_adj_list, axis=0))\n",
        "plt.plot(np.mean(SE_w_list, axis=0))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELDlqkodYDA-"
      },
      "source": [
        "### ACC / AUC / F1 output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDKGoOcByKJc"
      },
      "source": [
        "# arr_res = np.array(target_res)\n",
        "# # arr_res = np.array(target_res_baseline)\n",
        "\n",
        "# print(arr_res)\n",
        "# print(len(arr_res))\n",
        "# print(\"\\nModel: **%s**;\\nDataset: **%s**\\n\" % (model_type, dataset_name))\n",
        "# print(\"args_dropout: %.4f\" % args_dropout)\n",
        "# print(\"args_lr: %.6f\" % args_lr)\n",
        "# # print(\"args_weight_decay: %f\" % args_weight_decay)\n",
        "# print(\"args_epochs: %d\\n\" % args_epochs)\n",
        "\n",
        "# no_atts_list = [ \"DBLPE\"]\n",
        "# if dataset_name in no_atts_list:\n",
        "#     print(\"|Time step| ACC | AUC | F1 |\")\n",
        "#     print(\"|:--- |:--- |:------|:-----|\")\n",
        "#     for i in range(len(target_res)):\n",
        "#         print(\"|step #%d |%.6f | %.6f | %.6f|\" % (i, target_res[i][1], target_res[i][2], target_res[i][3])) # ACC, AUC, F1\n",
        "\n",
        "# else:\n",
        "#     print(\"| ACC | AUC | F1 |\")\n",
        "#     print(\"|:--- |:------|:-----|\")\n",
        "#     for i in range(len(arr_res)):\n",
        "#         print(\"|%.6f | %.6f | %.6f|\" % (arr_res[i][1], arr_res[i][2], arr_res[i][3])) # ACC, AUC, F1\n",
        "#     print(\"\\nAVG: \")\n",
        "#     print((np.sum(arr_res, axis=0)/len(arr_res))[1:])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUnUy5RjYcCx"
      },
      "source": [
        "##### RNNGCN_SE_back / Mean\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSvelbYDp21s"
      },
      "source": [
        "# tot_timestep = adj.shape[1]\n",
        "# model_type = \"RNNGCN_SE_back\"\n",
        "\n",
        "# # w_all = np.array(SE_2ws_adj_list) # weights\n",
        "# w_all = np.array(SE_w_list)\n",
        "# w_mean = np.mean(w_all, axis=0)\n",
        "# print(dataset_name, \", attention\", w_mean)\n",
        "# # tot_timestep = Features.shape[1]\n",
        "# tot = w_all.shape[0] \n",
        "# x = range(tot_timestep)\n",
        "\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Attention Weights (adj)')\n",
        "# plt.grid(True)    \n",
        "# plot_w_all, = plt.plot(x, w_mean, label=\"attention weights\")\n",
        "# plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "# plt.legend(handles=[plot_w_all])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mpv7bnS6DlNp"
      },
      "source": [
        "# while 1:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3V4MpgPAGopZ"
      },
      "source": [
        "### Masking attention in trained GCN / GraphSage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUNRNLoVGdmA"
      },
      "source": [
        "### Mask one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CM3acec4GoYi"
      },
      "source": [
        "# def eval_test(original_features, original_adj, original_labels, idx_train, idx_val, idx_test, model_type,normalize=False):\n",
        "#     t=time.time()\n",
        "#     lambda_matrix=None \n",
        "#     total_loss=0\n",
        "#     total_acc=0\n",
        "#     total_norm=[]    \n",
        "\n",
        "#     #choose adj matrix\n",
        "#     #GCN:n*n, Others: n*t*n\n",
        "    \n",
        "#     if dataset_name == \"Brain\" or dataset_name == \"simulated\":\n",
        "#         meta_timestep = 12\n",
        "#     else:\n",
        "#         meta_timestep = 10\n",
        "\n",
        "#     attention_w = dict()\n",
        "#     # attention_w = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "#     attention_w['simulated'] = [0.05907838, 0.05908475, 0.05908549, 0.0590926,  0.05907808, 0.05907955,\n",
        "#                                 0.05909814, 0.05977109, 0.06782387, 0.13779935, 0.16049674, 0.16051194]\n",
        "#     attention_w[\"None\"] = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "    \n",
        "#     attention_w[\"DBLP3\"] = [0.13257283, 0.11761762, 0.09207272, 0.09068672, \n",
        "#                             0.13907257, 0.08077238, 0.07735384, 0.08926542, \n",
        "#                             0.07579254, 0.10479335]\n",
        "#     attention_w[\"DBLP5\"] = [0.08490249, 0.07058566, 0.07522732, 0.093458354, 0.1454923, 0.13680391, 0.08714645, 0.11872233, 0.12575606, 0.061905116]\n",
        "#     attention_w[\"Reddit\"] = [0.10407882, 0.087858014, 0.103267744, 0.11869549, 0.111122675, 0.11745782, 0.09139953, 0.08404839, 0.089088246, 0.09298327]\n",
        "#     attention_w[\"Brain\"] = [0.10703684, 0.08153439, 0.06159713, 0.10095412,\n",
        "#                              0.05306721, 0.07350213, 0.12013485, 0.08990081,\n",
        "#                              0.05808821, 0.10553119, 0.07727117, 0.07138196]\n",
        "\n",
        "#     adj = original_adj\n",
        "#     features = original_features\n",
        "#     labels = original_labels\n",
        "\n",
        "#     print(\"adj shape:\", adj.shape)\n",
        "#     test_results_list = []\n",
        "#     for spec_t in range(meta_timestep):\n",
        "#         adj = original_adj\n",
        "#         features = original_features\n",
        "#         labels = original_labels\n",
        "    \n",
        "#         att_w = np.array(attention_w[dataset_name])\n",
        "#         att_w[spec_t] = 0.\n",
        "#         att_w /= np.sum(att_w)\n",
        "#         print(\"att_w\", att_w)\n",
        "\n",
        "#         if model_type=='GCN':  \n",
        "#             now_adj=att_w[0] * adj[:,0,:].clone()\n",
        "#             for i in range(1,adj.shape[1]):  #time_steps\n",
        "#                 now_adj+= att_w[i] * adj[:,i,:].clone()\n",
        "#             adj=now_adj\n",
        "#             features=features[:,-1,:]\n",
        "#         elif model_type=='GraphSage':\n",
        "#             now_adj=att_w[0] * adj[:,0,:].clone()\n",
        "#             for i in range(1,adj.shape[1]):  #time_steps\n",
        "#                 now_adj+= att_w[i] * adj[:,i,:].clone()\n",
        "#             adj=now_adj\n",
        "#             features=features[:,-1,:]\n",
        "\n",
        "#         #define model\n",
        "#         if model_type=='GCN':\n",
        "#             model = GCN(nfeat=features.shape[1],\n",
        "#                     nhid=args_hidden,\n",
        "#                     nclass=class_num,\n",
        "#                     dropout=args_dropout)\n",
        "#             model.load_state_dict(torch.load(\"./Eval_Mask_Model.tar\"))\n",
        "#             model.eval()\n",
        "#         elif model_type == 'GraphSage':\n",
        "#             adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
        "#             model = GraphSage(nfeat=features.shape[1],\n",
        "#                     nhid=args_hidden,\n",
        "#                     nclass=class_num,\n",
        "#                     dropout=args_dropout)\n",
        "#             model.load_state_dict(torch.load(\"./Eval_Mask_Model.tar\"))\n",
        "#             model.eval()\n",
        "        \n",
        "        \n",
        "#         if args_cuda:\n",
        "#             if model_type != 'EGCN':\n",
        "#                 model=model.to(torch.device('cuda:0'))#.cuda()\n",
        "#                 features = features.cuda()\n",
        "#                 adj = adj.to(torch.device('cuda:0'))\n",
        "#                 labels = labels.cuda()\n",
        "#                 idx_train = idx_train.cuda()\n",
        "#                 idx_val = idx_val.cuda()\n",
        "#                 idx_test = idx_test.cuda()\n",
        "\n",
        "#         # #optimizer and train\n",
        "#         # optimizer = optim.Adam(model.parameters(),\n",
        "#         #                         lr=args_lr, weight_decay=args_weight_decay)\n",
        "#         # Train model\n",
        "#         t_total = time.time()\n",
        "\n",
        "#         # Testing\n",
        "#         loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "#         test_results = [loss, acc, auc, f1]\n",
        "#         test_results_list.append(test_results[1])\n",
        "\n",
        "#         print(\"masking timestep\", spec_t, \":\", str(test_results[1])+'\\t'+str(test_results[2])+'\\t'+str(test_results[3]))#,end='\\t')\n",
        "#         # try:\n",
        "#         #     spec_norm=getKlargestSigVec(now_adj-Probability_matrix,2)[0]\n",
        "#         # except:\n",
        "#         #     spec_norm=0 #temperal adj\n",
        "\n",
        "#         del model\n",
        "#     return test_results_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WDWQ7TxGih3"
      },
      "source": [
        "### Mask all but one"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pT5yy1BwGiNY"
      },
      "source": [
        "# def eval_test_oneleft(original_features, original_adj, original_labels, idx_train, idx_val, idx_test, model_type,normalize=False):\n",
        "#     t=time.time()\n",
        "#     lambda_matrix=None \n",
        "#     total_loss=0\n",
        "#     total_acc=0\n",
        "#     total_norm=[]    \n",
        "\n",
        "#     #choose adj matrix\n",
        "#     #GCN:n*n, Others: n*t*n\n",
        "    \n",
        "#     if dataset_name == \"Brain\" or dataset_name == \"simulated\":\n",
        "#         meta_timestep = 12\n",
        "#     else:\n",
        "#         meta_timestep = 10\n",
        "\n",
        "#     attention_w = dict()\n",
        "#     attention_w['simulated'] = [0.05907838, 0.05908475, 0.05908549, 0.0590926,  0.05907808, 0.05907955,\n",
        "#                                 0.05909814, 0.05977109, 0.06782387, 0.13779935, 0.16049674, 0.16051194]\n",
        "#     attention_w[\"None\"] = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "    \n",
        "#     attention_w[\"DBLP3\"] = [0.13257283, 0.11761762, 0.09207272, 0.09068672, \n",
        "#                             0.13907257, 0.08077238, 0.07735384, 0.08926542, \n",
        "#                             0.07579254, 0.10479335]\n",
        "#     attention_w[\"DBLP5\"] = [0.08490249, 0.07058566, 0.07522732, 0.093458354, 0.1454923, 0.13680391, 0.08714645, 0.11872233, 0.12575606, 0.061905116]\n",
        "#     attention_w[\"Reddit\"] = [0.10407882, 0.087858014, 0.103267744, 0.11869549, 0.111122675, 0.11745782, 0.09139953, 0.08404839, 0.089088246, 0.09298327]\n",
        "#     attention_w[\"Brain\"] = [0.10703684, 0.08153439, 0.06159713, 0.10095412,\n",
        "#                              0.05306721, 0.07350213, 0.12013485, 0.08990081,\n",
        "#                              0.05808821, 0.10553119, 0.07727117, 0.07138196]\n",
        "\n",
        "#     adj = original_adj\n",
        "#     features = original_features\n",
        "#     labels = original_labels\n",
        "\n",
        "#     print(\"adj shape:\", adj.shape)\n",
        "#     test_results_list = []\n",
        "#     for spec_t in range(meta_timestep):\n",
        "#         adj = original_adj\n",
        "#         features = original_features\n",
        "#         labels = original_labels\n",
        "    \n",
        "#         att_w = np.array(attention_w[dataset_name])\n",
        "#         for mask_t in range(meta_timestep):\n",
        "#             if mask_t != spec_t:\n",
        "#                 att_w[mask_t] = 0\n",
        "#         att_w /= np.sum(att_w)\n",
        "#         print(\"att_w\", att_w)\n",
        "\n",
        "#         if model_type=='GCN':  \n",
        "#             now_adj=att_w[0] * adj[:,0,:].clone()\n",
        "#             for i in range(1,adj.shape[1]):  #time_steps\n",
        "#                 now_adj+= att_w[i] * adj[:,i,:].clone()\n",
        "#             adj=now_adj\n",
        "#             features=features[:,-1,:]\n",
        "#         elif model_type=='GraphSage':\n",
        "#             now_adj=att_w[0] * adj[:,0,:].clone()\n",
        "#             for i in range(1,adj.shape[1]):  #time_steps\n",
        "#                 now_adj+= att_w[i] * adj[:,i,:].clone()\n",
        "#             adj=now_adj\n",
        "#             features=features[:,-1,:]\n",
        "\n",
        "#         #define model\n",
        "#         if model_type=='GCN':\n",
        "#             model = GCN(nfeat=features.shape[1],\n",
        "#                     nhid=args_hidden,\n",
        "#                     nclass=class_num,\n",
        "#                     dropout=args_dropout)\n",
        "#             model.load_state_dict(torch.load(\"./Eval_Mask_Model.tar\"))\n",
        "#             model.eval()\n",
        "#         elif model_type == 'GraphSage':\n",
        "#             adj=dgl.from_networkx(nx.Graph(adj.numpy())) #fit in dgl\n",
        "#             model = GraphSage(nfeat=features.shape[1],\n",
        "#                     nhid=args_hidden,\n",
        "#                     nclass=class_num,\n",
        "#                     dropout=args_dropout)\n",
        "#             model.load_state_dict(torch.load(\"./Eval_Mask_Model.tar\"))\n",
        "#             model.eval()\n",
        "        \n",
        "        \n",
        "#         if args_cuda:\n",
        "#             if model_type != 'EGCN':\n",
        "#                 model=model.to(torch.device('cuda:0'))#.cuda()\n",
        "#                 features = features.cuda()\n",
        "#                 adj = adj.to(torch.device('cuda:0'))\n",
        "#                 labels = labels.cuda()\n",
        "#                 idx_train = idx_train.cuda()\n",
        "#                 idx_val = idx_val.cuda()\n",
        "#                 idx_test = idx_test.cuda()\n",
        "                \n",
        "#         # #optimizer and train\n",
        "#         # optimizer = optim.Adam(model.parameters(),\n",
        "#         #                         lr=args_lr, weight_decay=args_weight_decay)\n",
        "#         # Train model\n",
        "#         t_total = time.time()\n",
        "\n",
        "#         # Testing\n",
        "#         loss, acc, auc, f1 = test(model, features, adj, labels, idx_test)\n",
        "#         test_results = [loss, acc, auc, f1]\n",
        "#         test_results_list.append(test_results[1])\n",
        "\n",
        "#         print(\"masking timestep\", spec_t, \":\", str(test_results[1])+'\\t'+str(test_results[2])+'\\t'+str(test_results[3]))#,end='\\t')\n",
        "#         # try:\n",
        "#         #     spec_norm=getKlargestSigVec(now_adj-Probability_matrix,2)[0]\n",
        "#         # except:\n",
        "#         #     spec_norm=0 #temperal adj\n",
        "\n",
        "#         del model\n",
        "#     return test_results_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMYfGOodaSIU"
      },
      "source": [
        "### Mask one on Simulated data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7mj1QkTaaiJ"
      },
      "source": [
        "# #simulated data: setting of data generation\n",
        "\n",
        "# def generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector):\n",
        "    \n",
        "#     transit_matrix=[]\n",
        "#     for i in range(class_num):\n",
        "#         transit_one=[epsilon_vector[i]]*i+[1-epsilon_vector[i]]+[epsilon_vector[i]]*(class_num-1-i)\n",
        "#         transit_matrix+=[transit_one]\n",
        "#     #print((number_of_nodes*link_inclass_prob*epsilon_vector[0])**0.5)\n",
        "    \n",
        "#     adj=torch.zeros(number_of_nodes,Time_steps,number_of_nodes) #n*t*n adj matrix\n",
        "\n",
        "#     #assign initial labels\n",
        "#     labels=torch.randint(0,class_num,(number_of_nodes,)) #assign random label with equal probability\n",
        "#     labels=labels.to(dtype=torch.long)\n",
        "#     #label_node, speed up the generation of edges\n",
        "#     label_node_dict=dict()\n",
        "\n",
        "#     for j in range(class_num):\n",
        "#         label_node_dict[j]=[]\n",
        "\n",
        "#     for i in range(len(labels)):\n",
        "#         label_node_dict[int(labels[i])]+=[int(i)]\n",
        "\n",
        "#     total_labels=torch.zeros(number_of_nodes,Time_steps)\n",
        "#     #generate graph\n",
        "#     for i in range(int(Time_steps)):\n",
        "#         # print(\"for timestep %d, p = %.4f\" % (i, time_vector[i]))\n",
        "#         #change node\n",
        "#         change_nodes=[]\n",
        "#         for j in range(len(labels)):\n",
        "#             if random.random() < epsilon_vector[labels[j]]:\n",
        "#                 #less than change probability\n",
        "#                 tmp=int(labels[j])\n",
        "#                 #print(j)\n",
        "#                 while(1): #change label\n",
        "#                     labels[j]=torch.tensor(int(torch.randint(0,class_num,(1,))[0]))\n",
        "#                     if labels[j]!=tmp:\n",
        "#                         change_nodes+=[j]\n",
        "#                         break\n",
        "#                 #labels[j]=torch.tensor(not tmp)\n",
        "#         total_labels[:,i]=labels.clone()\n",
        "#         label_node_dict=dict()\n",
        "#         for j in range(class_num):\n",
        "#             label_node_dict[j]=[]\n",
        "\n",
        "#         for j in range(len(labels)):\n",
        "#             label_node_dict[int(labels[j])]+=[int(j)]\n",
        "#         #\n",
        "#         #generate symmetrix adj matrix at each time step\n",
        "#         for node_id in range(number_of_nodes):\n",
        "#             j=labels[node_id]\n",
        "#             for l in label_node_dict:\n",
        "#                 if l==j:\n",
        "#                     for z in label_node_dict[l]:  #z>node_id,  symmetrix matrix, no repeat\n",
        "#                         if z>node_id and random.random()<link_inclass_prob:\n",
        "#                             adj[node_id,i,z]= 1\n",
        "#                             adj[z,i,node_id]= 1\n",
        "#                 else:\n",
        "#                     for z in label_node_dict[l]:\n",
        "#                         if z>node_id and random.random()<link_outclass_prob:\n",
        "#                             adj[node_id,i,z]= 1\n",
        "#                             adj[z,i,node_id]= 1\n",
        "\n",
        "#     #generate feature use eye matrix\n",
        "#     features=torch.zeros(number_of_nodes,Time_steps,number_of_nodes)\n",
        "#     for i in range(features.shape[1]):\n",
        "#         features[:,i,:]=torch.eye(features.shape[0],features.shape[2])\n",
        "\n",
        "#     #seprate train,val,test\n",
        "#     idx_train = torch.LongTensor(range(number_of_nodes//5))\n",
        "#     idx_val = torch.LongTensor(range(number_of_nodes//5, number_of_nodes//2))\n",
        "#     idx_test = torch.LongTensor(range(number_of_nodes//2, number_of_nodes))\n",
        "\n",
        "#     #probability matrix at last time_step\n",
        "#     Probability_matrix=torch.zeros(number_of_nodes,number_of_nodes)\n",
        "#     for j in range(number_of_nodes):\n",
        "#         for k in range(number_of_nodes):\n",
        "#             if j==k:\n",
        "#                 continue\n",
        "#             elif labels[j]==labels[k]:\n",
        "#                 Probability_matrix[j][k]=link_inclass_prob\n",
        "#             else:\n",
        "#                 Probability_matrix[j][k]=link_outclass_prob\n",
        "\n",
        "#     return features.float(), adj.float(), total_labels.long(), idx_train, idx_val, idx_test, Probability_matrix\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsH2-wTFaRqg"
      },
      "source": [
        "# mode=\"simulated\"\n",
        "\n",
        "# if mode == 'simulated':   \n",
        "#     target_res = []\n",
        "#     SE_w_list = []\n",
        "#     SE_w = None\n",
        "#     matrix_w_list = []\n",
        "#     matrix_w = None\n",
        "#     SE_2ws_adj_list = []\n",
        "#     SE_2ws_x_list = []\n",
        "\n",
        "#     target_res_baseline = []\n",
        "#     SE_w_list_baseline = []\n",
        "#     matrix_w_list_baseline = []\n",
        "#     acc_drop_list = []\n",
        "\n",
        "#     for i in range(10):\n",
        "#         print(\"=\"*20)\n",
        "#         dataset_name='simulated'\n",
        "#         number_of_nodes=200\n",
        "#         Time_steps = 12\n",
        "#         class_num = 4\n",
        "#         link_inclass_prob = 20/number_of_nodes  #when calculation , remove the link in itself\n",
        "#         #EGCN good when network is dense 20/number_of_nodes  #fails when network is sparse. 20/number_of_nodes/5\n",
        "\n",
        "#         link_outclass_prob=link_inclass_prob/20\n",
        "#         # epsilon_vector=[10/number_of_nodes,20/number_of_nodes, 40/number_of_nodes, 50/number_of_nodes]\n",
        "#         epsilon_vector=[0.05, 0.10, 0.20, 0.25]\n",
        "#         # epsilon_vector=[0.8, 0.8, 0.8, 0.8]\n",
        "\n",
        "#         time_vector = [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]\n",
        "#         # special_timestep = [5]\n",
        "\n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_glitch(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep)   \n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_2stage(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, special_timestep)   \n",
        "#         features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_totallabel(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector)               \n",
        "#         # features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix = generate_data_totallabel_fixed(number_of_nodes, Time_steps, class_num, link_inclass_prob, link_outclass_prob, epsilon_vector, time_vector)               \n",
        "#         total_adj=adj\n",
        "#         total_labels=labels\n",
        "\n",
        "#         tot_timestep = total_labels.shape[1]\n",
        "#         print(\"adj:\", total_adj.shape)\n",
        "#         print(\"labels:\", total_labels.shape)\n",
        "#         print(\"timestep:\", tot_timestep)\n",
        "#         print(\"idx_train:\", idx_train.shape)\n",
        "#         print(\"idx_test:\", idx_test.shape)\n",
        "#         print(\"idx_val:\", idx_val.shape)\n",
        "\n",
        "#         args_hidden = class_num\n",
        "#         args_dropout = 0.5\n",
        "#         args_weight_decay = 5e-4    \n",
        "#         args_lr = 0.0025\n",
        "#         args_epochs = 500 \n",
        "#         # args_no_cuda=True\n",
        "#         args_no_cuda=False\n",
        "#         args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "#         print(\"CUDA:\", args_cuda)\n",
        "#         args_normalize=True\n",
        "#         print(dataset_name)\n",
        "\n",
        "#         # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "#         # model_type = 'RNNGCN_SE_2ws'\n",
        "#         model_type = 'GCN'\n",
        "#         # model_type = 'RNNGCN_SE_back'\n",
        "\n",
        "#         for target_time in range(tot_timestep-1, tot_timestep):\n",
        "#             print(target_time,end='\\t')\n",
        "#             adj = total_adj[:,:target_time+1,:]\n",
        "#             labels = total_labels[:,target_time]\n",
        "      \n",
        "#             res_one, SE_w, matrix_w = test_real_dataset()\n",
        "#             target_res.append(res_one)\n",
        "#             if \"TRNNGCN_SE\" in model_type:\n",
        "#                 matrix_w_list.append(matrix_w.detach().detach().cpu().numpy())\n",
        "#             if \"RNNGCN_SE\" in model_type:\n",
        "#                 if \"2ws\" in model_type:\n",
        "#                     SE_2ws_adj_list.append(SE_2ws_adj.detach().cpu().numpy())\n",
        "#                     SE_2ws_x_list.append(SE_2ws_x.detach().cpu().numpy())\n",
        "#                 elif SE_w != None:\n",
        "#                     SE_w_list.append(SE_w.detach().cpu().numpy())            \n",
        "#             print(' ',end='\\n')\n",
        "\n",
        "#         eval_res = eval_test_oneleft(features, adj, labels, idx_train, idx_val, idx_test, model_type,normalize=args_normalize)\n",
        "#         acc_drop = res_one[1] - np.array(eval_res)\n",
        "#         print(\"acc_drop:\", acc_drop)\n",
        "#         acc_drop_list.append(acc_drop) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTFRq5Kmugod"
      },
      "source": [
        "# tot_timestep = adj.shape[1]\n",
        "# model_type = \"RNNGCN_SE_2ws\"\n",
        "# if \"2ws\" in model_type: \n",
        "#     w_all = np.array(SE_2ws_adj_list) # weights\n",
        "#     # w_all = np.array(SE_w_list)\n",
        "#     w_mean = np.mean(w_all, axis=0)\n",
        "#     print(dataset_name, \", adj:\", w_mean)\n",
        "#     # tot_timestep = Features.shape[1]\n",
        "#     tot = w_all.shape[0]\n",
        "#     x = range(tot_timestep)\n",
        "    \n",
        "#     plt.xlabel('Timesteps')\n",
        "#     plt.ylabel('Attention Weights (adj)')\n",
        "#     plt.grid(True)    \n",
        "#     plot_w_all, = plt.plot(x, w_mean, label=\"attention for adj\")\n",
        "#     plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "#     plt.legend(handles=[plot_w_all])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtceHfvgl63T"
      },
      "source": [
        "# while 1:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kj9ZbJ6CGn10"
      },
      "source": [
        "### Evaluation: training & testing and output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xcVrJIJEYvq"
      },
      "source": [
        "#### Output attention W"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJmVIBkpFzWk"
      },
      "source": [
        "# tot_timestep = adj.shape[1]\n",
        "\n",
        "# # w_all = np.array(SE_2ws_adj_list) # for RNNGCN_SE_2ws\n",
        "# w_all = np.array(SE_w_list) # weights\n",
        "# w_mean = np.mean(w_all, axis=0)\n",
        "\n",
        "# print(dataset_name, \", acc_drop\")\n",
        "# print(\"[\"+\", \".join(str(bit) for bit in w_mean)+\"]\")\n",
        "\n",
        "# # tot_timestep = Features.shape[1]\n",
        "# tot = w_all.shape[0] \n",
        "# x = range(tot_timestep)\n",
        "\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Attention Weights (adj)')\n",
        "# plt.grid(True)    \n",
        "# plot_w_all, = plt.plot(x, w_mean, label=\"attention weights\")\n",
        "# plt.ylim(-0.02 + np.min(w_mean), 0.02 + np.max(w_mean))\n",
        "# plt.legend(handles=[plot_w_all])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awf3jONxWKRl"
      },
      "source": [
        "# # print(eval_res)\n",
        "# # print(res_one[1])\n",
        "# # acc_drop = res_one[1] - np.array(eval_res)\n",
        "# # print(\"acc_drop:\", acc_drop)\n",
        "\n",
        "\n",
        "# # w_all = np.array(SE_2ws_adj_list) # for RNNGCN_SE_2ws\n",
        "# w_all = np.array(SE_w_list) # weights\n",
        "# # print(\"[\"+\", \".join(str(bit) for bit in w_all)+\"]\")\n",
        "# # print(SE_w_list)\n",
        "# tot_timestep = w_all.shape[1]\n",
        "# tot_num = w_all.shape[0]\n",
        "# x = range(tot_timestep)\n",
        "\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Weights Learned (adj)')\n",
        "# plt.grid(True)    \n",
        "# plt.ylim(-0.01 + np.min(w_all), 0.01 + np.max(w_all))\n",
        "\n",
        "\n",
        "# for i in range(tot_num):\n",
        "#     w_i = w_all[i, ...]\n",
        "#     plt.plot(x, w_i)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RgEtz98ERc3"
      },
      "source": [
        "#### Output Acc Drop\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdg8Dr6AW34j"
      },
      "source": [
        "tot_timestep = adj.shape[1]\n",
        "\n",
        "# w_all = np.array(SE_2ws_adj_list) # weights\n",
        "print(len(acc_drop_list))\n",
        "w_all = np.array(acc_drop_list)\n",
        "\n",
        "# w_mean = np.mean(w_all, axis=1)\n",
        "# print(w_mean.shape)\n",
        "\n",
        "# bi = np.argmax(w_mean)\n",
        "# print(w_all[bi])\n",
        "# w_all = np.delete(w_all, bi, 0)\n",
        "# print(w_all.shape)\n",
        "\n",
        "# ci = np.argmin(w_mean)\n",
        "# print(w_all[ci])\n",
        "# w_all = np.delete(w_all, ci, 0)\n",
        "# print(w_all.shape)\n",
        "\n",
        "w_mean = np.mean(w_all, axis=0)\n",
        "\n",
        "print(dataset_name, \", acc_drop\")\n",
        "print(\"[\"+\", \".join(str(bit) for bit in w_mean)+\"]\")\n",
        "# tot_timestep = Features.shape[1]\n",
        "tot = w_all.shape[0] \n",
        "x = range(tot_timestep)\n",
        "\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Attention Weights (adj)')\n",
        "plt.grid(True)    \n",
        "plot_w_all, = plt.plot(x, w_mean, label=\"attention weights\")\n",
        "plt.ylim(-0.002 + np.min(w_mean), 0.002 + np.max(w_mean))\n",
        "plt.legend(handles=[plot_w_all])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqwiN2lTXr4B"
      },
      "source": [
        "w_all = np.array(acc_drop_list)\n",
        "print(acc_drop_list, len(acc_drop_list))\n",
        "# w_all = np.array(SE_w_list) # weights\n",
        "\n",
        "w_mean = np.mean(w_all, axis=1)\n",
        "print(w_mean.shape)\n",
        "\n",
        "# bi = np.argmax(w_mean)\n",
        "# print(w_all[bi])\n",
        "# w_all = np.delete(w_all, bi, 0)\n",
        "# print(w_all.shape)\n",
        "\n",
        "# ci = np.argmin(w_mean)\n",
        "# print(w_all[ci])\n",
        "# w_all = np.delete(w_all, ci, 0)\n",
        "# print(w_all.shape)\n",
        "\n",
        "tot_timestep = w_all.shape[1]\n",
        "tot_num = w_all.shape[0]\n",
        "x = range(tot_timestep)\n",
        "\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Weights Learned (adj)')\n",
        "plt.grid(True)    \n",
        "plt.ylim(-0.01 + np.min(w_all), 0.01 + np.max(w_all))\n",
        "\n",
        "\n",
        "for i in range(tot_num):\n",
        "    w_i = w_all[i, ...]\n",
        "    plt.plot(x, w_i)  \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOfMEVRmZRs0"
      },
      "source": [
        "while 1:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES0Idgi30e8f"
      },
      "source": [
        "### Evaluation: masking out each channel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5klkCFHFPMD"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N1uvShkr0Zw7"
      },
      "source": [
        "# mode = 'real'\n",
        "\n",
        "# if mode=='real':\n",
        "#     SE_2ws_adj = None\n",
        "#     SE_2ws_x = None\n",
        "\n",
        "#     SE_2ws_adj_list = []\n",
        "#     SE_2ws_x_list = []\n",
        "#     target_res = []\n",
        "#     SE_w_list = []\n",
        "#     SE_w = None\n",
        "#     matrix_w_list = []\n",
        "#     matrix_w = None\n",
        "#     target_res_baseline = []\n",
        "#     SE_w_list_baseline = []\n",
        "#     matrix_w_list_baseline = []\n",
        "\n",
        "#     all_drop_list = []\n",
        "\n",
        "#     for i in range(15):\n",
        "#         dataset_name=\"DBLP3\"\n",
        "#         # dataset_name=\"DBLP5\"\n",
        "#         # dataset_name=\"Reddit\"\n",
        "#         # dataset_name=\"Brain\"\n",
        "#         # dataset_name=\"reality_call\"\n",
        "#         # dataset_name=\"political_retweet\"\n",
        "\n",
        "#         features_origin, adj_origin, labels_origin, idx_train, idx_val, idx_test, Probability_matrix = load_real_data(dataset_name) \n",
        "#         class_num=int(labels_origin.max())+1\n",
        "#         print(\"class_num =\", class_num)\n",
        "#         # total_adj=adj_origin\n",
        "#         # total_labels=labels_origin\n",
        "#         print(\"total_adjsize: \", adj_origin.shape)\n",
        "#         print(\"total_labelsize: \", labels_origin.shape)\n",
        "#         print(\"features size: \", features_origin.shape)\n",
        "\n",
        "#         # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "#         # model_type = 'RNNGCN_SE_2ws'\n",
        "#         # model_type = 'RNNGCN_SE'\n",
        "#         # model_type = 'original_RNNGCN'\n",
        "#         # model_type = 'GraphSage'\n",
        "#         # model_type = 'GCN'\n",
        "#         model_type = 'GCNLSTM'\n",
        "\n",
        "#         args_hidden = class_num\n",
        "#         # args_hidden = 10\n",
        "#         args_dropout = 0.5\n",
        "#         args_weight_decay = 5e-4    \n",
        "#         args_lr = 0.0025\n",
        "#         args_epochs = 500 \n",
        "#         # args_no_cuda=True\n",
        "#         args_no_cuda=False\n",
        "#         args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "#         print(\"CUDA:\", args_cuda)\n",
        "#         args_normalize=True\n",
        "#         print(dataset_name)\n",
        "        \n",
        "#         no_atts_list = [\"DBLPE_importance\", \"DBLPE\", \"sparse_DBLPE\"]\n",
        "#         fixed_no_atts_list = [\"hospital\", \"reality_call\", \"political_retweet\"]\n",
        "\n",
        "#         meta_time_step = 10\n",
        "#         mean_drop = np.zeros(meta_time_step)\n",
        "#         single_drop_list = []\n",
        "\n",
        "#         mean_drop_t = 0\n",
        "\n",
        "#         for t in range(meta_time_step):\n",
        "#             print(' ---------- %d ----------' % t) \n",
        "        \n",
        "#             # Without masking:\n",
        "#             features = features_origin\n",
        "#             adj = adj_origin\n",
        "#             labels = labels_origin\n",
        "\n",
        "#             res_one, SE_w, matrix_w = test_real_dataset()\n",
        "#             target_res_baseline.append(res_one)\n",
        "#             drop_acc_res_baseline = res_one[1]\n",
        "            \n",
        "\n",
        "#             # Applying a mask...\n",
        "#             eval_mask = []\n",
        "#             eval_mask.extend(range(t))\n",
        "#             eval_mask.extend(range(t+1, meta_time_step))\n",
        "\n",
        "#             adj = adj_origin[:, eval_mask, :]\n",
        "#             features = features_origin[:, eval_mask, :]\n",
        "#             # No masks for labels except for DBLPE\n",
        "#             if dataset_name in no_atts_list:            # DBLPE\n",
        "#                 labels = labels_origin[eval_mask, :]\n",
        "#             # elif dataset_name in fixed_no_atts_list:    # hospital etc\n",
        "#             #     total_label = labels[:, eval_mask, :]\n",
        "#             # else:                                       # Brain etc\n",
        "#             #     total_label = labels[:, eval_mask, :]\n",
        "\n",
        "#             # Masking complete, start running..\n",
        "\n",
        "#             res_one, SE_w, matrix_w = test_real_dataset()\n",
        "#             target_res.append(res_one)\n",
        "#             drop_acc_res = res_one[1]\n",
        "#             drop_this_time = drop_acc_res_baseline - drop_acc_res\n",
        "#             print(\"#%d: acc drop %.4f\" % (i, drop_this_time))\n",
        "#             single_drop_list.append(drop_this_time)\n",
        "#             print(\"len of single:\", len(single_drop_list))\n",
        "#         all_drop_list.append(single_drop_list)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UMu7BP2LWzzd"
      },
      "source": [
        "# drop_arr = np.array(all_drop_list)\n",
        "# print(drop_arr.shape)\n",
        "# print(model_type, \",\", dataset_name, \":\")\n",
        "# print(np.mean(drop_arr, axis=0))\n",
        "# x = range(meta_time_step)\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Acc drop: ' + model_type + \" / \" + dataset_name)\n",
        "# plt.grid(True)    \n",
        "# plot_w_all, = plt.plot(x, np.mean(drop_arr, axis=0))\n",
        "# # plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "# plt.legend(handles=[plot_w_all])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTN8IjrVLzCi"
      },
      "source": [
        "# arr_res = np.array(target_res)\n",
        "# arr_res_baseline = np.array(target_res_baseline)\n",
        "\n",
        "# print(arr_res)\n",
        "# print(len(arr_res))\n",
        "# print(\"\\nModel: **%s**;\\nDataset: **%s**\\n\" % (model_type, dataset_name))\n",
        "# print(\"args_dropout: %.4f\" % args_dropout)\n",
        "# print(\"args_lr: %.6f\" % args_lr)\n",
        "# # print(\"args_weight_decay: %f\" % args_weight_decay)\n",
        "# print(\"args_epochs: %d\\n\" % args_epochs)\n",
        "\n",
        "# print(\"| ACC drop | AUC drop | F1 drop |\")\n",
        "# print(\"|:--- |:------|:-----|\")\n",
        "# for i in range(len(arr_res)):\n",
        "#     print(\"|%.6f | %.6f | %.6f|\" % (arr_res_baseline[i][1] - arr_res[i][1], \n",
        "#                                     arr_res_baseline[i][2] - arr_res[i][2], \n",
        "#                                     arr_res_baseline[i][3] - arr_res[i][3])) # ACC, AUC, F1\n",
        "# print(\"\\nAVG: \")\n",
        "# print((np.sum(arr_res, axis=0)/len(arr_res))[1:])\n",
        "\n",
        "\n",
        "# # no_atts_list = [ \"DBLPE\"]\n",
        "# # if dataset_name in no_atts_list:\n",
        "# #     print(\"|Time step| ACC | AUC | F1 |\")\n",
        "# #     print(\"|:--- |:--- |:------|:-----|\")\n",
        "# #     for i in range(len(target_res)):\n",
        "# #         print(\"|step #%d |%.6f | %.6f | %.6f|\" % (i, target_res[i][1], target_res[i][2], target_res[i][3])) # ACC, AUC, F1\n",
        "\n",
        "# # else:\n",
        "#     # print(\"| ACC | AUC | F1 |\")\n",
        "#     # print(\"|:--- |:------|:-----|\")\n",
        "#     # for i in range(len(arr_res)):\n",
        "#     #     print(\"|%.6f | %.6f | %.6f|\" % (arr_res[i][1], arr_res[i][2], arr_res[i][3])) # ACC, AUC, F1\n",
        "#     # print(\"\\nAVG: \")\n",
        "#     # print((np.sum(arr_res, axis=0)/len(arr_res))[1:])\n",
        "\n",
        "# drop_arr = np.array(arr_res_baseline[:, 1] - arr_res[:, 1])\n",
        "# x = range(meta_time_step)\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Acc drop')\n",
        "# plt.grid(True)    \n",
        "# plot_w_all, = plt.plot(x, drop_arr)\n",
        "# # plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "# plt.legend(handles=[plot_w_all])    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RPJV6xYHzt"
      },
      "source": [
        "### Weight Curve (all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Twr2dXUe9SuW"
      },
      "source": [
        "##### 2ws: adj_w and x_w / Mean"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alL65yvRYnu8"
      },
      "source": [
        "# tot_timestep = adj.shape[1]\n",
        "# model_type = \"RNNGCN_SE_2ws\"\n",
        "# if \"2ws\" in model_type: \n",
        "#     w_all = np.array(SE_2ws_adj_list) # weights\n",
        "#     # w_all = np.array(SE_w_list)\n",
        "#     w_mean = np.mean(w_all, axis=0)\n",
        "#     print(dataset_name, \", adj:\", w_mean)\n",
        "#     # tot_timestep = Features.shape[1]\n",
        "#     tot = w_all.shape[0]\n",
        "#     x = range(tot_timestep)\n",
        "    \n",
        "#     plt.xlabel('Timesteps')\n",
        "#     plt.ylabel('Attention Weights (adj)')\n",
        "#     plt.grid(True)    \n",
        "#     plot_w_all, = plt.plot(x, w_mean, label=\"attention for adj\")\n",
        "#     plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "#     plt.legend(handles=[plot_w_all])    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB-Jr3kXj98o"
      },
      "source": [
        "# if \"2ws\" in model_type: \n",
        "#     w_all = np.array(SE_2ws_x_list) # weights\n",
        "#     w_mean = np.mean(w_all, axis=0)\n",
        "\n",
        "#     # tot_timestep = Features.shape[1]\n",
        "#     tot = w_all.shape[0]\n",
        "#     x = range(tot_timestep)\n",
        "    \n",
        "#     print(dataset_name, \", x:\", w_mean)\n",
        "#     plt.xlabel('Timesteps')\n",
        "#     plt.ylabel(\"Attention Weights (x)\")\n",
        "#     plt.grid(True)    \n",
        "#     plot_w_all, = plt.plot(x, w_mean, label=\"attention for x\")\n",
        "#     plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "#     plt.legend(handles=[plot_w_all])        \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QP-FuJYJ9OaI"
      },
      "source": [
        "##### All:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwZqQK3Rj_m-"
      },
      "source": [
        "# if \"2ws\" in model_type:\n",
        "#     w_all = np.array(SE_2ws_adj_list)\n",
        "#     # w_all = np.array(SE_w_list) # weights\n",
        "#     print(SE_w_list)\n",
        "#     tot_timestep = w_all.shape[1]\n",
        "#     tot_num = w_all.shape[0]\n",
        "#     x = range(tot_timestep)\n",
        "\n",
        "#     plt.xlabel('Timesteps')\n",
        "#     plt.ylabel('Weights Learned (adj)')\n",
        "#     plt.grid(True)    \n",
        "#     plt.ylim(-0.01 + np.min(w_all), 0.01 + np.max(w_all))\n",
        "\n",
        "\n",
        "#     for i in range(tot_num):\n",
        "#         w_i = w_all[i, ...]\n",
        "#         plt.plot(x, w_i)  \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBEvAljykAuu"
      },
      "source": [
        "# if \"2ws\" in model_type:\n",
        "#     w_all = np.array(SE_2ws_x_list)\n",
        "#     # w_all = np.array(SE_w_list_baseline) # weights\n",
        "#     # print(SE_w_list)\n",
        "#     tot_timestep = w_all.shape[1]\n",
        "#     tot_num = w_all.shape[0]\n",
        "#     x = range(tot_timestep)\n",
        "\n",
        "#     plt.xlabel('Timesteps')\n",
        "#     plt.ylabel('Weights Learned (x)')\n",
        "#     plt.grid(True)    \n",
        "#     plt.ylim(-0.01 + np.min(w_all), 0.01 + np.max(w_all))\n",
        "\n",
        "\n",
        "#     for i in range(tot_num):\n",
        "#         w_i = w_all[i, ...]\n",
        "#         plt.plot(x, w_i)          "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7_y7hKzgZmfb"
      },
      "source": [
        "# while 1:\n",
        "#     pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUvjFzlrYhQj"
      },
      "source": [
        "#### Other models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rS7cLL2Be2O1"
      },
      "source": [
        "no_atts_list = [\"DBLPE_importance\", \"DBLPE\", \"sparse_DBLPE\", \"hospital\", \"retweet\"]\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if \"SE\" in model_type:\n",
        "    if dataset_name in no_atts_list:\n",
        "        # x = range(14)\n",
        "        timestep = 14\n",
        "        x = range(timestep)\n",
        "        weights = SE_w_list[timestep-1] \n",
        "        # weights = SE_w_list_basline[timestep-1]\n",
        "        assert len(weights) == timestep, \"weights len: %d \" % len(weights)\n",
        "        weights_y = weights\n",
        "        print(weights_y)\n",
        "\n",
        "        stat = get_label_stat(total_labels) \n",
        "        stat_y = stat / np.sum(stat[:timestep])\n",
        "        stat_y = [None] + list(stat_y)\n",
        "        stat_y = stat_y[:len(weights)]\n",
        "        \n",
        "        plot_w, = plt.plot(x, weights_y, label=\"weights\")\n",
        "\n",
        "        plt.legend(handles=[plot_w, plot_stat])\n",
        "        plt.xlabel('Timesteps')\n",
        "        plt.ylabel('Weights Learned')\n",
        "        \n",
        "        plt.grid(True)\n",
        "        # plt.ylim(0.05, 0.16)\n",
        "\n",
        "    else:\n",
        "        w_all = np.array(SE_w_list)\n",
        "        # w_all = np.array(SE_w_list_baseline) # weights\n",
        "        # print(SE_w_list)\n",
        "        tot_timestep = w_all.shape[1]\n",
        "        tot_num = w_all.shape[0]\n",
        "        x = range(tot_timestep)\n",
        "\n",
        "        plt.xlabel('Timesteps')\n",
        "        plt.ylabel('Weights Learned')\n",
        "        plt.grid(True)    \n",
        "        plt.ylim(-0.01 + np.min(w_all), 0.01 + np.max(w_all))\n",
        "\n",
        "\n",
        "        for i in range(tot_num):\n",
        "            w_i = w_all[i, ...]\n",
        "            plt.plot(x, w_i)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-LZwhv1_uPg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZSA9_fCzYN4B"
      },
      "source": [
        "### Weight Curve (mean)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pgHIx7lfGzS"
      },
      "source": [
        "# target res\n",
        "tot_timestep = adj.shape[1]\n",
        "\n",
        "if target_res_baseline != []:\n",
        "    if \"SE\" in model_type:\n",
        "        if dataset_name != \"DBLPE\" or dataset_name != \"DBLPE_importance\":    \n",
        "            w_all = np.array(SE_w_list) # weights\n",
        "            w_mean = np.mean(w_all, axis=0)\n",
        "\n",
        "            w_all_baseline = np.array(SE_w_list_baseline) # weights\n",
        "            w_mean_baseline = np.mean(w_all_baseline, axis=0)  \n",
        "\n",
        "            # tot_timestep = Features.shape[1]\n",
        "            tot = w_all.shape[0]\n",
        "            x = range(tot_timestep)\n",
        "            \n",
        "            plt.xlabel('Timesteps')\n",
        "            plt.ylabel('Weights Learned')\n",
        "            plt.grid(True)    \n",
        "            plot_w_all, = plt.plot(x, w_mean, label=\"with pe\")\n",
        "            plot_w_all_baseline, = plt.plot(x, w_mean_baseline, label=\"without pe\")\n",
        "            plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "            plt.legend(handles=[plot_w_all, plot_w_all_baseline])\n",
        "else:\n",
        "    if \"SE\" in model_type:\n",
        "        if dataset_name != \"DBLPE\" or dataset_name != \"DBLPE_importance\":    \n",
        "            w_all = np.array(SE_w_list) # weights\n",
        "            w_mean = np.mean(w_all, axis=0)\n",
        "\n",
        "            # tot_timestep = Features.shape[1]\n",
        "            tot = w_all.shape[0]\n",
        "            x = range(tot_timestep)\n",
        "            \n",
        "            plt.xlabel('Timesteps')\n",
        "            plt.ylabel('Weights Learned')\n",
        "            plt.grid(True)    \n",
        "            plot_w_all, = plt.plot(x, w_mean, label=model_type)\n",
        "            plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "            plt.legend(handles=[plot_w_all])    \n",
        "    else:\n",
        "        x = range(len(target_res))\n",
        "        plt.xlabel('Timesteps')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.grid(True)    \n",
        "        y = []\n",
        "        \n",
        "        for i in target_res:\n",
        "            y.append(i[1])\n",
        "        curve1,  = plt.plot(x, y, label=model_type)    \n",
        "        # plt.ylim(0, 0.05 + np.max(w_all))\n",
        "        plt.legend(handles=[curve1], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mai34OfkyvhH"
      },
      "source": [
        "# weight_exp = np.array(w_mean)\n",
        "\n",
        "# print(weight_exp.shape)\n",
        "# print(np.sum(weight_exp))\n",
        "\n",
        "# exp_name = \"EVL_\" + dataset_name + \"_\" + model_type + \"_\" + str(len(weight_exp)) +\".npy\"\n",
        "# with open(exp_name, 'wb') as f:\n",
        "#     np.save(f, weight_exp)\n",
        "# print(exp_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7J5y8hmIqgK"
      },
      "source": [
        "# %cp ./EVL_Brain_RNNGCN_SE_12.npy /content/Colab/Clustering-RGCN/\n",
        "# %ls  /content/Colab/Clustering-RGCN/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GBKu2idFc7J1"
      },
      "source": [
        "while True:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vjU0HXtxkHQw"
      },
      "source": [
        "# w_all = np.array(SE_w_list) # weights\n",
        "# print(SE_w_list)\n",
        "# tot_timestep = w_all.shape[1]\n",
        "# tot_num = w_all.shape[0]\n",
        "# x = range(tot_timestep)\n",
        "\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Weights Learned')\n",
        "# plt.grid(True)    \n",
        "# plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "\n",
        "\n",
        "# for i in range(tot_num):\n",
        "#     w_i = w_all[i, ...]\n",
        "#     plt.plot(x, w_i)\n",
        "\n",
        "# x = range(len(target_res))\n",
        "# plt.xlabel('Timesteps')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.grid(True)    \n",
        "# y = []\n",
        "\n",
        "# for i in target_res:\n",
        "#     y.append(i[1])\n",
        "# curve1,  = plt.plot(x, y, label=model_type)    \n",
        "# # plt.ylim(0, 0.05 + np.max(w_all))\n",
        "# plt.legend(handles=[curve1], loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh_WfurtInDt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBAl8KrRJtSh"
      },
      "source": [
        "# DBLPE plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2J_iAHuVZZ9"
      },
      "source": [
        "baseline_res_list = []\n",
        "baseline_SE_w_all = []\n",
        "# baseline_model_list = [\"RNNGCN_SE_back\", \"original_RNNGCN\", \"RNNGCN_SE_back_pe\"]\n",
        "baseline_model_list = [\"GCN\"]\n",
        "# Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "\n",
        "dataset_name=\"DBLPE\"\n",
        "features, adj, labels, idx_train, idx_val, idx_test, Probability_matrix=load_real_data(dataset_name)\n",
        "class_num=int(labels.max())+1\n",
        "print(\"class_num =\", class_num)\n",
        "total_adj=adj\n",
        "total_labels=labels\n",
        "print(\"total_adjsize: \", total_adj.shape)\n",
        "print(\"total_labelsize: \", total_labels.shape)\n",
        "print(\"features size: \", features.shape)\n",
        "\n",
        "for model_name_i in baseline_model_list:\n",
        "    baseline_res = []\n",
        "    SE_w_list = []\n",
        "    SE_w = 0\n",
        "    mode=\"real\"\n",
        "    print(model_name_i)\n",
        "\n",
        "    model_type = model_name_i\n",
        "    torch.autograd.set_detect_anomaly(True)\n",
        "\n",
        "    # Options: GCN, GAT, GraphSage #dynamic_spec, DynAERNN #GCNLSTM, EGCN, RNNGCN, TRNNGCN\n",
        "    args_hidden = class_num\n",
        "    args_dropout = 0.5\n",
        "    args_lr = 0.0025\n",
        "    # args_lr = 0.025\n",
        "    args_weight_decay = 5e-4\n",
        "    # args_epochs = 500\n",
        "    args_epochs = 500 \n",
        "\n",
        "    # args_no_cuda=True\n",
        "    args_no_cuda=False\n",
        "    \n",
        "    args_cuda = not args_no_cuda and torch.cuda.is_available()\n",
        "    print(\"CUDA:\", args_cuda)\n",
        "    args_normalize=True\n",
        "\n",
        "    print(dataset_name)\n",
        "\n",
        "    if mode=='real':\n",
        "        if dataset_name==\"DBLPE\":\n",
        "            #target_time=13 #0-13\n",
        "            for target_time in range(0, 14):\n",
        "                print(target_time)\n",
        "                adj = total_adj[:,:target_time+1,:]\n",
        "                labels = total_labels[:,target_time]\n",
        "                res_one, SE_w, matrix_w = test_real_dataset()     \n",
        "                if \"SE\" in model_type:\n",
        "                    print(\"SE is here!\")\n",
        "                    SE_w_list.append(SE_w.detach().cpu().numpy())\n",
        "                baseline_res.append(res_one)\n",
        "\n",
        "                # gpu_memory_log()\n",
        "                print(' ',end='\\n')\n",
        "\n",
        "\n",
        "    print(\"\\nModel: **%s**;\\nDataset: **%s**\\n\" % (model_type, dataset_name))\n",
        "    print(\"args_dropout: %.4f\" % args_dropout)\n",
        "    print(\"args_lr: %.6f\" % args_lr)\n",
        "    # print(\"args_weight_decay: %f\" % args_weight_decay)\n",
        "    print(\"args_epochs: %d\\n\" % args_epochs)\n",
        "\n",
        "    print(\"|Time step| ACC | AUC | F1 |\")\n",
        "    print(\"|:--- |:--- |:------|:-----|\")\n",
        "    for i in range(len(baseline_res)):\n",
        "        print(\"|step #%d |%.6f | %.6f | %.6f|\" % (i, baseline_res[i][1], \n",
        "                                                  baseline_res[i][2], \n",
        "                                                  baseline_res[i][3])) # ACC, AUC, F1    \n",
        "    baseline_res_list.append(baseline_res)\n",
        "    baseline_SE_w_all.append(SE_w_list)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-re4Uc4-Vl8M"
      },
      "source": [
        "\n",
        "if dataset_name == \"DBLPE\":\n",
        "    plt.xlabel('Timesteps')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.grid(True) \n",
        "    print(len(baseline_res_list))\n",
        "    x = range(len(baseline_res_list[0]))\n",
        "\n",
        "    # target_y = np.array(target_res)[:, 1]\n",
        "    baseline_y = [None] * 3\n",
        "    # Only plotting Acc\n",
        "    for i in range(len(baseline_res_list)):\n",
        "        baseline_y[i] = np.array(baseline_res_list[i])[:, 1]\n",
        "\n",
        "        \n",
        "    # assert len(baseline_res[0]) == len(target_res), \"%d, %d\" % (len(baseline_res[0]), len(target_res))\n",
        "    \n",
        "    # curve_target,  = plt.plot(x, target_y, label=\"RNNGCN_SE_decay\")\n",
        "    # curve_legend = [curvet_target]\n",
        "    curve_legend = []\n",
        "    for i in range(len(baseline_res_list)):\n",
        "        curve_i,  = plt.plot(x, baseline_y[i], label=baseline_model_list[i])    \n",
        "        curve_legend.append(curve_i)\n",
        "    plt.legend(handles=curve_legend, loc='lower right')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWBvUA3Ac_UW"
      },
      "source": [
        "SE_w_list = baseline_SE_w_all[0]\n",
        "\n",
        "w_all = np.array(SE_w_list[-1]) # weights\n",
        "\n",
        "# tot_timestep = w_all.shape[1]\n",
        "tot_num = w_all.shape[0]\n",
        "# x = range(tot_timestep)\n",
        "x = range(tot_num)\n",
        "\n",
        "plt.xlabel('Timesteps')\n",
        "plt.ylabel('Weights Learned')\n",
        "plt.grid(True)    \n",
        "plt.ylim(-0.02 + np.min(w_all), 0.02 + np.max(w_all))\n",
        "\n",
        "print(len(baseline_SE_w_all[0]))\n",
        "curve_legend = []\n",
        "curve_0, = plt.plot(x, baseline_SE_w_all[0][-1], label=baseline_model_list[0])\n",
        "curve_1, = plt.plot(x, baseline_SE_w_all[2][-1], label=baseline_model_list[2])\n",
        "curve_legend.append(curve_0)\n",
        "curve_legend.append(curve_1)\n",
        "\n",
        "plt.legend(handles=curve_legend, loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}